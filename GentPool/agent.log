INFO:root:Creating Config from file: ./gentpool/pool/assign_2_scholar/agent.yaml
INFO:root:Starting to load configuration from ./gentpool/pool/assign_2_scholar/agent.yaml
INFO:root:Successfully loaded configuration from ./gentpool/pool/assign_2_scholar/agent.yaml
INFO:root:--------------------
INFO:root:[green]Welcome to Gentopia!
INFO:root:--------------------
INFO:root:[green]User: 
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:{"name":"pdf_text_extract", "arguments":{"url":"https://arxiv.org/pdf/2201.05966"}}

INFO:root:Calling function: pdf_text_extract ...
INFO:root:Done
INFO:root:--------------------
INFO:root:UNIFIED SKG: Unifying and Multi-Tasking Structured Knowledge
Grounding with Text-to-Text Language Models
Tianbao Xie∗1Chen Henry Wu∗2Peng Shi3Ruiqi Zhong4Torsten Scholak5
Michihiro Yasunaga6Chien-Sheng Wu7Ming Zhong8Pengcheng Yin9Sida I. Wang10
Victor Zhong17Bailin Wang11Chengzu Li12Connor Boyle17Ansong Ni13Ziyu Yao14
Dragomir Radev13Caiming Xiong7Lingpeng Kong1,12Rui Zhang15
Noah A. Smith16,17Luke Zettlemoyer10,17Tao Yu1,17
1The University of Hong Kong2Carnegie Mellon University3University of Waterloo
4UC Berkeley5ServiceNow Research6Stanford University7Salesforce Research
8UIUC9Google Research10Facebook AI Research11University of Edinburgh
12Shanghai AI Lab13Yale University14George Mason University15Penn State University
16Allen Institute for Artiﬁcial Intelligence17University of Washington
Abstract
Structured knowledge grounding (SKG) lever-
ages structured knowledge to complete user
requests, such as semantic parsing over
databases and question answering over knowl-
edge bases. Since the inputs and outputs
of SKG tasks are heterogeneous, they have
been studied separately by different communi-
ties, which limits systematic and compatible
research on SKG. In this paper, we overcome
this limitation by proposing the U NIFIED SKG
framework, which uniﬁes 21 SKG tasks into
a text-to-text format, aiming to promote sys-
tematic SKG research, instead of being exclu-
sive to a single task, domain, or dataset. We
use U NIFIED SKG to benchmark T5 with dif-
ferent sizes and show that T5, with simple
modiﬁcations when necessary, achieves state-
of-the-art performance on almost all of the 21
tasks. We further demonstrate that multi-task
preﬁx-tuning improves the performance on
most tasks, largely improving the overall per-
formance. U NIFIED SKG also facilitates the
investigation of zero-shot and few-shot learn-
ing, and we show that T0, GPT-3, and Codex
struggle in zero-shot and few-shot learning
for SKG. We also use U NIFIED SKG to con-
duct a series of controlled experiments on
structured knowledge encoding variants across
SKG tasks. U NIFIED SKG is easily extensible
to more tasks, and it is open-sourced at https:
//github.com/hkunlp/unifiedskg .1
1 Introduction
Structured knowledge (e.g., web tables, knowledge
graphs, and databases) stores large amounts of data
in organized structures, forming a basis for a wide
range of applications, e.g., medical diagnosis, per-
sonal assistants, and customer relations manage-
∗Equal contributions. Author contributions in App. A.
1Latest collections at https://unifiedskg.com .ment. Accessing and searching data in structured
knowledge typically requires mastering query lan-
guages through professional training. To promote
the efﬁciency of data access, structured knowledge
grounding (SKG) systems ground user requests
in structured knowledge and produce various out-
puts, including computer programs (e.g., SQL and
SPARQL), table cell values, and natural language
responses (Figure 1). For example, semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005) converts natural language questions
into formal programs; knowledge-base question an-
swering (Berant et al., 2013) derives answers from
tables or knowledge graphs.
SKG has attracted signiﬁcant interest and has
been studied through different tasks deﬁned by dif-
ferent communities. Recent developments in tasks,
models, and datasets for SKG have led to task-
speciﬁc modeling advances, making each task’s
progress seemingly unique and incompatible. A
main reason is that SKG tasks are heterogeneous .
Different types of structured knowledge, such as
databases or knowledge graphs, lead to highly spe-
cialized encoders (Lin et al., 2019; Herzig et al.,
2020; Wang et al., 2020; Yasunaga et al., 2021).
Some SKG tasks, e.g., semantic parsing, use cus-
tomized decoders to generate programs (Yin and
Neubig, 2018; Ren et al., 2021). Therefore, instead
of solving common challenges in SKG research,
improvements in SKG have been prone to be exclu-
sive to a single task, domain, or dataset.
In this paper, we propose the UNIFIED SKG
framework to advocate for a unifying view of 21
SKG tasks across six task families and multiple
data domains (Table 1). UNIFIED SKG standardizes
datasets, models, code, experiments, and evalua-
tion metrics into a single framework. By casting
user requests, structured knowledge, and outputsarXiv:2201.05966v3  [cs.CL]  18 Oct 2022147Structured Knowledge Grounding
knowledge graphs
web tables/pages
databases/apps
Structured KnowledgeGreece held its last Summer Olympics in which year?Question AnsweringDescribe the table result.Data-to-Text GenerationCanada obtained 3 more gold medals than Mexico.Fact Veriﬁcation      I am looking for a cheap                 restaurant in the city center.      Book a table for 8 at 18:30 on         Thursday.Dialogs       Which players did win the         Australian Open?Semantic ParsingSELECT T1.name FROM players AS T1 JOIN matches AS T2 ON T1.id = T2.winner_id WHERE T2.Tourney = “Australian Open” SQL/SPARQL/s-Expression2014Answer setIn 1970, Hawaii's population mainly consists of 38.8% White and 57.7% Asian, Native Hawaiian...NL descriptionFalseBooleanRestaurant(price=cheap,area=center)Restaurant(price=cheap,area=center,               name=Dojo Noodle Bar,              people=8, time=18:30,            day=Thursday)Multi-turn SQL-like programsUniﬁedSKGFigure 1: Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests. By
casting inputs and outputs into the text-to-text format, U NIFIED SKG standardizes datasets, models, code, experi-
ments, and metrics for 21 SKG tasks.
into the text-to-text format (Raffel et al., 2020), it
promotes model advances where new tasks can be
framed with our standardized abstraction, and new
models can be easily applied to diverse SKG tasks.
While previous works also cast SKG tasks into the
text-to-text format (Hosseini-Asl et al., 2020; Shaw
et al., 2021; Liu et al., 2021), their independent
choices of pretrained language models (PLMs),
input-output formats, and frameworks make our
uniﬁcation non-trivial. UNIFIED SKG is easily ex-
tensible to more SKG tasks, and it is open-sourced
to promote community-wide progress.
Using UNIFIED SKG as a benchmark, we show
that ﬁnetuning T5 (with constrained decoding or
reranking when necessary) on individual tasks
achieves state-of-the-art (sota) results on almost
all of the 21 tasks, establishing a powerful and
reproducible starting point for SKG research. T5
performance also increases with size on most tasks.
UNIFIED SKG facilitates multi-task learning on
SKG, enabling knowledge sharing and cross-task
generalization. Although simple multi-task learn-
ing has mixed results, we show that multi-task
learning with preﬁx-tuning (Li and Liang, 2021)
beneﬁts most tasks and largely improves the overall
performance, on both T5-base and T5-large.
UNIFIED SKG is a challenging testbed for few-
shot (Brown et al., 2020; Ye et al., 2021a) and
zero-shot learning (Zhong et al., 2021; Wei et al.,
2021; Sanh et al., 2021) with PLMs. Our experi-
ments show that models like T0 (Sanh et al., 2021)
struggle in zero-shot learning on SKG tasks, and
GPT-3 (Brown et al., 2020) and Codex (Chen et al.,
2021a) struggle in few-shot learning on SKG tasks.
UNIFIED SKG enables a series of controlled ex-periments on structured knowledge encoding. We
ﬁnd that T5 is sensitive to encoding variations, and
the sensitivity varies across tasks. UNIFIED SKG
aims to facilitate more general and robust struc-
tured knowledge encoding methods. Finally, we
conduct a comprehensive error analysis across
SKG tasks. Although the errors made by PLMs
decrease with the model size, T5-3B may still gen-
erate invalid outputs.
In summary, we 1) unify and benchmark 21 SKG
tasks under the UNIFIED SKG framework to evalu-
ate diverse grounding goals and structured knowl-
edge sources, 2) demonstrate (near) sota perfor-
mance of T5 on all the uniﬁed SKG tasks, using
a single, general-purpose approach, 3) show the
beneﬁt of knowledge sharing across SKG tasks
via multi-task preﬁx-tuning, and 4) analyze recent
modeling contributions (zero-shot, few-shot, and
structured knowledge encoding) on these tasks. We
hope UNIFIED SKG enables the design of new mod-
els and learning algorithms that generalize to di-
verse SKG tasks and to identify their challenges.
2 Related Work
SKG with PLMs PLMs have been applied to sev-
eral SKG tasks. To encode structured knowledge,
prior work linearized the structured knowledge and
concatenated it with the text (Hwang et al., 2019;
Liu et al., 2020; Hosseini-Asl et al., 2020; Liu et al.,
2021), which has been augmented by positional
encoding (e.g., row/column embedding) (Herzig
et al., 2020; Yin et al., 2020a) and template-based
linearization (Chen et al., 2020a,b; Oguz et al.,
2021), and planning (Su et al., 2021). Recently,
cell-column alignment is modeled by manipulatingTask Family Task Knowledge Input User Input Output
Semantic ParsingSpider (Yu et al., 2018) Database Question SQL
GrailQA (Gu et al., 2021) Knowledge Graph Question s-Expression
WebQSP (Yih et al., 2016) Knowledge Graph Question s-Expression
MTOP (Li et al., 2021) API Calls Question TOP Representation
Question AnsweringWikiSQL (Zhong et al., 2017) Table Question Answer
WikiTQ (Pasupat and Liang, 2015) Table Question Answer
CompWebQ (Talmor and Berant, 2018) Knowledge Graph Question Answer
HybridQA (Chen et al., 2020c) Table + Text Passage Question Answer
MultiModalQA (Talmor et al., 2021) Table + Text + Image Question Answer
FeTaQA (Nan et al., 2021a) Table Question Free-Form Answer
Data-to-TextDART (Nan et al., 2021b) Triple None Text
ToTTo (Parikh et al., 2020) Highlighted Table None Text
MultiWoZ (Budzianowski et al., 2018) Ontology Dialog Dialog State
KVRET (Eric et al., 2017) Table Dialog Response
Conversational SParC (Yu et al., 2019b) Database Multi turn SQL
CoSQL (Yu et al., 2019a) Database Dialog SQL
SQA (Iyyer et al., 2017) Table Multi turn Answer
Fact VeriﬁcationTabFact (Chen et al., 2020b) Table Statement Boolean
FEVEROUS (Aly et al., 2021) Table + Text Statement Boolean
Formal-Language-to-TextSQL2Text (Shu et al., 2021) Optional Database SQL Text
Logic2Text (Chen et al., 2020d) Table Schema Python-like program Text
Table 1: We unify 21 SKG tasks with different knowledge input, user input, and output, covering six task families.
the attention matrix of transformers (Zhang et al.,
2020; Eisenschlos et al., 2021). Hierarchical encod-
ing is another way to represent the structure, e.g.,
Wang et al. (2021b) used tree-based transformers
to represent the structure of the tables; Iida et al.
(2021) used transformers to encode row and col-
umn representations; Chen et al. (2021b) used hier-
archical transformers to encode KG triples. SKG’s
outputs include, but are not limited to, structured
meaning representations (e.g., logic forms, SQL),
dialogue states, natural language, answer sets, and
Boolean values. Among them, structured mean-
ing representation is challenging for PLMs because
they are originally trained on natural language. To
bridge this gap, Shin et al. (2021) adopted the in-
sights from Berant and Liang (2014) and Marzoev
et al. (2020) and proposed to convert formal lan-
guage into an English-like representation, decode
with GPT-3, and map back to formal language au-
tomatically. We do not focus on these techniques
in this work; instead, we unify all tasks and system-
atically compare them.
Task format uniﬁcation Recent years witnessed
the trend of unifying related but different tasks into
a shared format. McCann et al. (2018) uniﬁed vari-
ous tasks as question answering. Yin et al. (2020b)
and Wang et al. (2021a) uniﬁed few-shot learning
as textual entailment. PLUR (Chen et al., 2021c)
uniﬁed program learning, understanding, and repair
tasks into a graph-to-sequence format. In this paper,
we focus on the text-to-text format (Raffel et al.,
2020) due to its ﬂexibility. Different from unifyingtasks that only take text as input, a core challenge
in unifying SKG tasks into the text-to-text format
is to linearize structured knowledge. Notably, Uni-
ﬁedQA (Khashabi et al., 2020) uniﬁed QA tasks,
while UNIFIED SKG covers a broader scope of six
task families for systematic exploration.
Cross-task generalization with PLMs Multi-
task learning and transfer learning go beyond task
boundaries, view different tasks as related, and
have been shown to outperform single-task learning
(Aghajanyan et al., 2021a; Vu et al., 2021). Large
PLMs show potential for zero-shot and few-shot
learning, e.g., GPT-2 (Radford et al., 2019) and
GPT-3 (Brown et al., 2020), which can be improved
by multi-task learning (Zhong et al., 2021), e.g.,
FLAN (Wei et al., 2021), T0 (Sanh et al., 2021),
and CrossFit (Ye et al., 2021a). ExT5 (Aribandi
et al., 2021) shows that scaling up multi-task learn-
ing helps improve pretraining efﬁciency and down-
stream performances. UNIFIED SKG facilitates the
investigation of multi-task, zero-shot, and few-shot
learning on SKG tasks.
3 The U NIFIED SKG Framework
3.1 Task Uniﬁcation
The guiding principle of UNIFIED SKG’s task se-
lection is diversity. We unify 21 SKG tasks across
six task families and multiple domains (Table 1).
Our task families include:
•Semantic parsing converts questions to logical
forms (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005).TrueEntailedFalseRefutedBooleanNatural languageHow many singers are there?Formal languageselect count (*) from singerSet of answersDialogue states { New York, Paris } New York, Paris
hotel type none, hotel name Wartworthhotel type: nonehotel name: WartworthMars Hill College : joined : 1973 | Mars Hill College : location : Mars HillTablescol : player | no. row 1 : antonio| 21 row 2 : voshon| 2Triples(Mars Hill College, joined, 1973)(Mars Hill College, location, Mars Hill)playerno.antonio21voshon2hotel-pricerange: cheap, expensive, dontcare; Hotel-parking: free, no, yes, dontcarehotel-pricerangefrom {cheap, expensive dontcare}hotel-parking from {free, no, yes, dontcare}Cut Bank film.cinematographyBen Richardson | Cut Bank film.languageEnglishKnowledge graph OntologyCut Bank Ben RichardsonEnglishfilm.cinematographyfilm.language; structured knowledge:                ; context:User requestContextUnified output sequence !𝑦Unified input sequence !𝑥Text-to-text PLMLinearized structured knowledgeFigure 2: We unify SKG tasks with heterogeneous inputs and outputs into the text-to-text format.
•Question answering derives answers to natural
language questions based on structured data (Be-
rant et al., 2013).
•Data-to-text generation describes structured
data in natural language (Novikova et al., 2017).
•Fact veriﬁcation checks if a statement is true
based on the structured data (Chen et al., 2020b).
•Conversational tasks require understanding of
not only the user’s last request but also the full
interaction history between users and machines
(Budzianowski et al., 2018; Eric et al., 2019; Yu
et al., 2019a).
•Formal language to text translation describes
formal language in natural language (Chen et al.,
2020d).
All these tasks take as input xa user request, a
structured knowledge input, and an optional (di-
alogue) context to predict an output y. Figure 2
illustrates how we convert the input xto an in-
put sequence ˜xand the output yto an output se-
quence ˜yby means of “linearization” (Liu et al.,
2021), enabling the uniﬁcation of diverse forms of
structured knowledge. We provide more details,
examples, and input length analysis in the Appen-
dices F and G. Our code implementation uses Hug-
ging Face’s Transformers (Wolf et al., 2020) and
Datasets (Lhoest et al., 2021) toolkits.
3.2 Modeling
The simplest usage of UNIFIED SKG is to train on
individual tasks. In this case, we minimize the
negative log-likelihood loss averaged over tokens
in each batch. For decoding, we use beam search
by default. UNIFIED SKG also facilitates explo-
ration of multi-task learning, few-shot, and zero-
shot learning with PLMs, and details are presented
in the corresponding parts in Section 4.4 Experiments and Analysis
4.1 Results on Individual Tasks
We apply T5 models (Raffel et al., 2020) on each
individual task in UNIFIED SKG. For model train-
ing, we set the maximum number of epochs as
50–200, depending on the dataset size. We use
early stopping and model selection on the devel-
opment set. More details are shown in Appendix
D.1. For each task, we report one commonly used
metric in Table 2. See Appendix B for all metrics.
Comparison with previous sota Table 2 shows
that vanilla T5-3B outperforms most previous
sota models not trained on extra unsupervised in-
domain data. Some semantic parsing sota models,
denoted as+in Table 2, are also T5 with con-
strained decoding (Scholak et al., 2021) or rerank-
ing (Ye et al., 2021b). This shows that a generalist
architecture like T5, when scaled up to a certain
size, can be as good as task-speciﬁc architectures
for SKG, suggesting the potential of larger PLMs.
Model scalability In general, T5 performance
increases with the model size, but this trend varies
across task families. Semantic parsing, QA, and
fact veriﬁcation tasks get large beneﬁts from in-
creased sizes, while text generation does not. See
Section 4.5 for a human evaluation for text genera-
tion tasks. Also, the gap between T5-base (220M)
and T5-large (770M) is larger than the gap between
T5-large (770M) and T5-3B (3B).
Effect of pretraining on structured knowledge
Some smaller models pretrained on structured
knowledge (Liu et al., 2021) show competitive
performance as T5-3B, suggesting that pretrain-
ing with structured data is beneﬁcial for SKG. This
result calls for structured knowledge pretraining
that generalizes to different SKG tasks across do-
mains, which can be systematically explored using
UNIFIED SKG.Metric T5-base T5-large T5-3B Previous sota (w/o extra) Previous sota (w/ extra)
Spider (dev.) Match 58.12 66.63 71.76 75.5+(Scholak et al., 2021) 74.7 (Rubin and Berant, 2021)
GrailQA Match 62.39 67.30 70.11 83.8+(Ye et al., 2021b) —
WebQSP F1 78.83 79.45 80.70 83.6+(Ye et al., 2021b) —
MTOP Match 85.49 86.17 86.78 86.36 (Pasupat et al., 2021) —
WikiTQ Acc 35.76 43.22 49.29 44.5 (Wang et al., 2019) 57.5 (Liu et al., 2021)
WikiSQL Acc 82.63 84.80 85.96 85.8 (Liu et al., 2021) 89.5 (Liu et al., 2021)
CompWebQ Acc 68.43 71.38 73.26 70.4‡(Das et al., 2021) —
HybridQA (dev.) Acc 54.07 56.95 59.41 60.8‡(Eisenschlos et al., 2021) 63.4‡(Eisenschlos et al., 2021)
MultiModalQA (dev.) F1 75.51 81.84 85.28 82.7 (Yoran et al., 2021) 83.8 (Yoran et al., 2021)
FeTaQA BLEU 29.91 32.45 33.44 30.54 (Nan et al., 2021a) —
DART BLEU 46.22 46.89 46.66 46.89 (Nan et al., 2021b) 47.2 (Aghajanyan et al., 2021b)
ToTTo (dev.) BLEU 48.29 48.95 48.95 48.95 (Kale and Rastogi, 2020) —
MultiWoZ2.1 Joint Acc 54.64 54.45 55.42 60.61∗(Dai et al., 2021) 60.48 (Yu et al., 2021)
KVRET Micro F1 66.45 65.85 67.88 63.6 (Gou et al., 2021) —
SParC (dev.) Match 50.54 56.69 61.51 54.1 (Hui et al., 2021) 62.2 (Yu et al., 2021)
CoSQL (dev.) Match 42.30 48.26 54.08 56.9+(Scholak et al., 2021) 52.1 (Yu et al., 2021)
SQA Overall Acc 52.91 61.28 62.37 58.6 (Liu et al., 2021) 74.5 (Liu et al., 2021)
TabFact Acc 76.13 80.85 83.68 74.4 (Yang et al., 2020) 84.2 (Liu et al., 2021)
FEVEROUS (dev.) Acc 75.05 79.81 82.40 82.38 (Aly et al., 2021) —
SQL2Text BLEC 93.52 93.68 94.78 93.7 (Shu et al., 2021) —
Logic2Text BLEC 90.66 90.57 91.39 88.6 (Shu et al., 2021) —
Table 2: Test or development (dev.) set performance of models trained on individual tasks. Vanilla T5 or T5 with
simple modiﬁcations (e.g.,+constrained decoding or reranking) achieve sota on nearly all tasks. The best result
without extra pretraining is shown in bold . More detailed results and result variances can be found in Tables 11
and 12 in Appendix. Human evaluation for generation tasks is in Section 4.5. w/ (w/o) extra means with (without)
extra pretraining on unsupervised structured data (e.g., web tables).2
Spider WikiTQ DART MWoZ TabFact SQL2Text
T5-3B 71.76 50.65 50.38 58.46 83.97 92.71
T0-3B 68.09 50.62 50.16 60.20 85.51 92.93
Table 3: Comparison between T5-3B and T0-3B. T0-
3B is initialized from LM-adapted T5 and further pre-
trained on a large number of non-SKG tasks. We ﬁne-
tune both models on individual tasks. T0-3B under-
performs T5-3B on semantic parsing (Spider) and out-
performs T5-3B on dialogue state tracking (MWoZ)
and fact veriﬁcation (TabFact). We report results on
the dev. set.
Effect of pretraining on non-SKG tasks T0-3B
(Sanh et al., 2021) is initialized from T5-3B and
pretrained on multiple tasks that (in most cases) do
not use structured knowledge as input (non-SKG
tasks). Exploring the performance of T0-3B on
SKG tasks helps us understand the relationship
between SKG tasks and non-SKG tasks. Table 3
shows that T0-3B under-performs T5-3B on se-
mantic parsing and outperforms T5-3B on dialogue
state tracking and fact veriﬁcation. We note that
T0-3B is pretrained on dialogue QA, dialogue sum-
marization, and NLI tasks; therefore, pretraining
on non-SKG tasks might not be useful for SKG
unless we add similar SKG tasks to pretraining.
2For GrailQA and WebQSP, we run T5 and rerun the pre-
vious sota model (Ye et al., 2021b) using the gold entities. For4.2 Multi-Task Learning
UNIFIED SKG facilitates the exploration of multi-
task learning. In this part, we systematically study
multi-task learning on all 21 uniﬁed tasks. We ﬁnd
that SKG beneﬁts from multi-task preﬁx-tuning on
both T5-base and T5-large, showing that the bene-
ﬁts from multi-task learning is scalable in terms of
the model size. The baselines we use include:
Single-task ﬁnetuning (ST-F) , which is ﬁnetun-
ing on individual tasks, same as Section 4.1.
Single-task preﬁx-tuning (ST-P; Li and Liang,
2021 ), which learns lightweight task-speciﬁc pa-
MultiModalQA and FEVEROUS, we report performance of
T5 and the previous sota models on the dev. samples with at
least one table (samples with image input are further excluded
for MultiModalQA); The gold table and text candidates are
used for both T5 and previous sota (for MultiModelQA, num-
bers are from (Yoran et al., 2021), and for FEVEROUS, we
rerun the available model (Aly et al., 2021) on gold candi-
dates to obtain the number). We use sacreBLEU to report all
BLEU results.‡We use gold entity linking, but the previous
sota does not, which makes the results not directly compara-
ble; therefore, we do not bold any numbers for CompWebQ
and HybridQA.∗T5-base with the independent output scheme
(Lee et al., 2021) achieves 56.66 on MWoZ2.1, higher than our
sequence output scheme. For WebQSP, as the original dataset
does not have a dev. set, we split the original train set into
in-house train/dev. sets (90%/10%), following prior practice
(e.g. Ren et al. (2021)). Similarly, for CompWebQ, as the test
set is not publicly available, we split the original dev. set into
in-house dev./test sets (20%/80%). For GrailQA, we split the
original dev. set into in-house dev./test sets (5%/95%).rameters while keeping the PLM ﬁxed. We set the
preﬁx length as 10. Clive et al. (2021) also used
preﬁx-tuning on T5 for data-to-text generation.
Multi-task ﬁnetuning (MT-F) , which combines
the training data of all tasks with temperature mix-
ing (Raffel et al., 2020; after hyperparameter tuning
with a few steps, we set the temperature as 2). We
select model weights based on the average metric
on all tasks’ development set.
Table 4 shows that ST-P is comparable to ST-F
on nearly all tasks. However, we ﬁnd that it takes
about 5–10 times as many training steps (See Ap-
pendix E), which is similarly observed for prompt-
tuning (Lester et al., 2021). We also observe that
MT-F leads to mixed results. For many tasks, MT-F
is even worse than ST-F.
Multi-task preﬁx-tuning (MT-P) Our explana-
tion for the mixed results of MT-F is that the inputs
of SKG tasks contain different structured knowl-
edge from diverse domains, making it difﬁcult to
learn shared parameters effectively. To address this
challenge, we ﬁrst pretrain a preﬁx on all tasks,
freezing T5 and using the same temperature mix-
ing as MT-F. In the second step, we initialize each
task’s preﬁx with this pretrained preﬁx and opti-
mize the preﬁx while freezing T5. This initializa-
tion step is similar to the prompt transfer explored
in Vu et al. (2021). Following ST-P, we set the
preﬁx length as 10.
Table 4 shows that multi-task preﬁx-tuning out-
performs single-task ﬁnetuning and single-task
preﬁx-tuning on most tasks, and it largely outper-
forms the naive multi-task learning baseline. It
demonstrates that SKG tasks can be studied to-
gether to share data and knowledge.
Exploring task knowledge transfer UNI-
FIED SKG facilitates studying knowledge transfer
between SKG tasks. Given two tasks, task A and
task B , we ﬁrst train the model on task A and then
continue training on task B. Table 5 shows that
tasks beneﬁt from other tasks with the same data
source (e.g., tasks that all use Wikipedia tables as
structured knowledge). We do not observe posi-
tive transfer between parallel tasks (e.g., semantic
parsing tasks with different structured knowledge
and different output) and subtask (e.g., question
answering can be viewed as the execution semantic
parses) when data sources are different. Compared
to the positive results in Table 4, results in this part
indicate that manually selecting source and target
tasks may not be efﬁcient for multi-task learning.T5-base T5-large
ST-F ST-P MT-F MT-P ST-F MT-P
Spider 58.12 58.61 58.90 59.86 66.63 67.60
GrailQA 60.00 61.33 56.00 62.67 67.00 65.33
WebQSP 72.50 73.81 67.25 74.77 73.96 74.92
MTOP 83.89 82.93 78.79 82.77 84.70 84.34
WikiTQ 36.94 36.42 41.15 39.74 43.30 50.90
WikiSQL 84.50 83.09 81.85 84.44 86.27 87.45
CompWQ 66.71 67.85 68.28 69.70 68.85 71.27
HybridQA 54.07 54.93 53.52 54.88 56.95 57.33
MMQA 75.51 75.50 76.63 76.40 81.84 84.59
FeTaQA 29.00 28.03 31.85 29.33 30.94 32.48
DART 50.62 50.33 49.74 50.68 51.72 50.82
ToTTo 48.29 45.70 45.29 45.21 48.95 47.90
MWoZ2.1 57.52 56.67 53.19 57.06 58.23 59.24
KVRET 20.04 19.68 18.53 21.32 18.84 20.76
SParC 50.54 51.04 51.70 51.29 56.69 59.02
CoSQL 42.30 44.39 43.59 45.68 48.26 51.64
SQA 49.49 44.81 51.48 48.43 59.12 58.15
TabFact 76.34 75.74 71.19 77.86 81.40 83.62
FEVER. 75.05 75.33 76.85 78.02 79.81 82.05
SQL2Text 93.69 94.50 93.57 93.79 93.35 93.93
Logic2Text 92.15 95.25 92.24 94.70 92.88 93.61
Total para. 21T T + 21P T T + 21P 21T T + 21P
Avg. score 60.82 60.76 60.08 61.84 64.27 65.57
Table 4: Multi-task learning results. ST and MT stand
for single-task and multi-task. F and P stand for ﬁne-
tuning and preﬁx-tuning. For total parameters, TandP
are the numbers of T5 and preﬁx parameters ( P≪T).
Multi-task learning with preﬁx improves the perfor-
mance on most tasks, largely improving the overall per-
formance. We report results on the dev. set.
Task A Task B Type B only A to B
WikiSQL TabFact same source 81.43 82.76
TabFact WikiTQ same source 43.30 45.88
WikiSQL FeTaQA same source 30.94 31.19
Spider GrailQA parallel tasks 67.00 67.00
Spider WikiTQ subtask 43.30 41.68
Spider TabFact weakly related 81.43 80.39
Table 5: Task knowledge transfer. We use T5-large
here. B only means training the model on task B; A to B
means to train the model on task A and then to ﬁnetune
the model on task B. In both settings, we report task
B’s development set performance. We ﬁnd that tasks
beneﬁt from other tasks with the same data source.
4.3 Zero-Shot and Few-Shot Learning
The text-to-text uniﬁcation of UNIFIED SKG en-
ables us to investigate zero/few-shot learning on
SKG with large PLMs.
Zero-shot learning setting Zero-shot learning
enables models to solve tasks with natural language
descriptions without training samples. We follow
T0 (Sanh et al., 2021) to create similar natural lan-
guage instructions for the unseen tasks. Our in-
structions are provided in Appendix D.3.
Few-shot learning settings Brown et al. (2020)
showed that large PLMs could be few-shot learnersT5-3B T0 3B GPT-3 175B Codex 175B
ﬁnetune zero-shot select random select random
Spider 71.76 0.00 20.00 18.33 3.78 40.72 43.23 4.16
WikiTQ 50.65 12.68 32.00 29.33 9.04 26.21 20.46 4.21
DART 50.38 23.42 40.23 34.21 4.50 42.13 36.54 1.67
MWoZ 58.46 0.00 18.00 0.02 0.02 23.47 0.06 0.03
TabFact 83.97 52.45 51.00 49.67 3.79 50.97 51.58 1.59
SQL2Text 92.71 39.64 94.00 85.00 2.65 90.64 88.31 1.61
Table 6: Zero-shot and few-shot learning for SKG. Sub-
scripts show the standard deviation with three runs. se-
lectmeans to select the most similar training samples as
few-shot examples, while random means to randomly
select training samples as few-shot examples. T0 per-
forms poorly on all the tasks in the zero-shot setting.
Codex outperforms GPT-3 on tasks that generate struc-
tured programs (Spider and MultiWoZ).
by encoding a few training samples as “context”
to learn without gradient updates. We use GPT-
3 (Brown et al., 2020) and Codex (Chen et al.,
2021a) to explore such few-shot learning for SKG.
To stay within our budget, for GPT-3, we report the
performance on 100 random dev. set samples. We
explore two settings for few-shot learning.
In the ﬁrst setting, we randomly sample few-shot
examples from the training set; these examples are
shared by all dev. set samples, denoted as random
in Table 6. For sequences that are too long for
Codex (4096) and GPT-3 (2048), we use as many
examples as possible and make sure that there is at
least one example (truncated if needed).
In the second setting, we follow Gao et al. (2021)
to select few-shot examples from the training set.
We call this setting few-shot with example selection ,
denoted as select in Table 6. We use the pretrained
SBERT (Reimers and Gurevych, 2020) for sen-
tence embeddings of the user request input (for
tasks that only have structured input, we embed the
linearized structured input) and sample ﬁve most
similar examples measured by cosine similarity.
Further details (e.g., prompts and task instructions)
are provided in Appendix D.4.
SKG is challenging for zero/few-shot learning.
Table 6 shows that zero-shot performance is very
poor on most tasks (Spider and MultiWoZ are
even 0). It also shows a large gap between few-
shot learning and ﬁnetuning for Spider, WikiTQ,
MWoZ, and TabFact, while the gap is smaller for
generation tasks. For few-shot learning, example
selection based on similarity outperforms random
selection, but the gap is usually smaller than 10
points out of 100. It is also interesting to compare
the results between synthesis tasks (Spider), which
requires predicting programs, and induction tasksSpider WikiTQ MultiWoZ2.1 TabFact
rs(c) 66.63 2.31 43.30 0.25 58.23 0.39 81.43 0.16
sr 64.12 38.78 — 80.98
rcs — — 58.89 —
Table 7: Ordering of inputs. Subscripts show the stan-
dard deviation with three runs. s,r, and cstand for
the structured knowledge, request input, and context.
Placing rbefore sis always better, and placing cbe-
tween randsis better for dialogue state tracking (Mul-
tiWoZ2.1).
Spider WikiTQ DART MultiWoZ2.1
Same Order 66.63 2.31 43.30 0.25 51.72 0.15 58.23 0.39
Reversed Order 64.80 37.80 48.47 13.59
Table 8: Order-sensitivity of structured knowledge.
Subscripts show the standard deviation with three runs.
Same Order is the default benchmark setting. Re-
versed Order means to reverse the structured knowl-
edge ordering on the development set (but not the train-
ing set). Tasks with cross-domain tables (in WikiTQ),
databases (in Spider), and triples (in DART) are less
order-sensitive, while pre-deﬁned ontology (in Multi-
WoZ2.1) is highly order-sensitive.
(WikiTQ and TabFact), where a model directly out-
puts answers (Devlin et al., 2017). We ﬁnd that
PLMs generally struggle more when adapting to
induction tasks (e.g., close to random-guess on the
binary classiﬁcation task TabFact), reminiscent of
recent attempts in program synthesis and induc-
tion using PLMs (Austin et al., 2021). For GPT-3
and Codex, better zero-shot performances can be
expected by better prompt design.
4.4 Structured Knowledge Encoding
Structured knowledge encoding has been widely
explored (Bogin et al., 2019; Lin et al., 2019; Agar-
wal et al., 2020; Saxena et al., 2020; Yasunaga and
Liang, 2020; Yasunaga et al., 2022; and others de-
tailed in Section 2). We hope that UNIFIED SKG
can promote systematic study of general structured
knowledge encoding. To this end, this part focuses
on the linearization of structured knowledge.
Does the order of user input, structured knowl-
edge, and context matter? To explore the effect
of the order of user input, structured knowledge,
and context, we rerun the single-task experiments
while switching the order of these components in
both the training and development set. Table 7
shows that placing the text before structured knowl-
edge ( rs) is better than the opposite ( sr), which is
consistent across SKG tasks. Our explanation is
that the position of the text is relatively ﬁxed in rs,Spider WikiSQL TabFact
Linearization 40.23 59.21 58.77
Natural Language 38.59 63.16 58.56
Table 9: Converting structured knowledge into natural
language for low-resource learning. A large improve-
ment is observed on question answering (WikiSQL),
but not on text2SQL semantic parsing (Spider) and fact
veriﬁcation (TabFact).
helping the decoder to learn stable attention over
the text. Also, placing the context in between the
text and structured knowledge yields better results.
Is T5 sensitive to structured knowledge order-
ing? Order-insensitivity is common for most struc-
tured knowledge, e.g., permutation of columns in
a table preserves the meaning. To study this in-
sensitivity, we evaluate T5-large on a manipulated
development set where the order of schema (for
database), column (for table), or slots and values
(for ontology) is reversed. Table 8 shows that tasks
with cross-domain tables and databases are less
order-sensitive, while models are very sensitive to
the order of ontology. Other types of robustness
(e.g., robustness to cell values irrelevant to the an-
swer) remain an open question in U NIFIED SKG.
Is it beneﬁcial to represent structured knowl-
edge as natural language? SKG data is not typi-
cally used to pretrain PLMs. Given ample training
data, PLMs adapt well to SKG tasks, as shown
in Table 2. However, under the low-resource set-
ting, converting structured data to natural language
might be helpful. For Spider, we use a shared tem-
plate to convert structured data to natural language.
For TabFact and WikiSQL, we randomly selected
236 tables shared by both datasets and manually
labeled templates to convert each row into a sen-
tence. Examples of the templates are shown in
Appendix I. These templates produce about 1000
samples for each task, divided into training and
test sets. We ﬁnd that, in WikiSQL, the conversion
to natural language stabilizes and accelerates the
training process. Table 9 shows that conversion
to natural language improves the performance on
WikiSQL, has no signiﬁcant inﬂuence on TabFact,
and slightly degrades the performance on Spider.
4.5 Human Evaluation for Generation Tasks
For each generation task, we randomly sample 100
development set samples and ask human annotators
to judge the correctness of each output, using a 0-1
score. Details are provided in Appendix D.5. TableMetric T5-base T5-large T5-3B
FeTaQABLEU 29.00 30.94 31.73
Human∗†36.0% 51.3% 57.3%
DARTBLEU 50.62 51.72 50.38
Human 90.7% 91.7% 87.7%
ToTToBLEU 48.29 48.95 48.95
Human 78.7% 80.0% 81.3%
KVRETBLEU 20.04 18.84 17.75
Human†72.3% 66.3% 75.0%
SQL2TextBLEC 93.69 93.35 92.71
Human∗83.7% 90.3% 84.7%
Logic2TextBLEC 92.15 92.88 91.69
Human†77.2% 81.5% 84.2%
Table 10: Automatic metrics and human evaluation on
the development set of generation tasks.∗p < 0.05
for “the rank-1 model is better than the rank-2 model”.
†p <0.05for “the rank-2 model is better than the rank-
3 model”. Automatic metrics do not always reﬂect hu-
man evaluation. Larger models are not always better.
10 shows that automatic metrics do not always re-
ﬂect human evaluation, calling for better automatic
metrics to truly reﬂect the model’s ability on gen-
eration tasks. Larger models are not always better,
and detailed error analysis is provided below.
4.6 Error Analysis
Error analysis based on output validity Uncon-
strained decoding from PLMs may generate invalid
outputs . For semantic parsing, we divide wrong
outputs into invalid outputs (i.e., not executable
when the output is SQL, and not parse-able when
the output is s-expression or TOP-representation)
andvalid but wrong answers . Figure 3 shows that,
for SQL semantic parsing, a large number of errors
are caused by invalid outputs, and the number of in-
valid outputs gradually decreases with the increase
of model size. This phenomenon is also observed
by Scholak et al. (2021), who used constrained de-
coding to improve the validity, largely improving
the parsing performance. For s-expression seman-
tic parsing, invalid outputs take up 30–50% of all
wrong outputs, and increasing the model size does
not reduce invalidity signiﬁcantly. For fact veriﬁ-
cation tasks, valid outputs are “entailed” and “re-
futed”. We observe that T5 always generates valid
outputs. For question answering, we do not include
the validity analysis since the validity check for an
answer is non-trivial and could be imprecise.
Error analysis for text generation tasks For
generation tasks, we consider four types of errors:
missing information (required information is notbase large 3b
Size0100200300CountError type
Invalid Valid-but-wrong(a) Spider
base large 3b
Size050100CountError type
Invalid Valid-but-wrong (b) GrailQA
base large 3b
Size01020304050ProportionError type
Miss. Inf.
Contra.Halluc.
Ungram.
(c) FeTaQA
base large 3b
Size0102030ProportionError type
Miss. Inf.
Contra.Halluc.
Ungram. (d) ToTTo
Figure 3: Error analysis. For semantic parsing, we
plot the number of invalid/valid-but-wrong predictions.
For generation, we plot the proportion of missing-
information/contradiction/hallucination/ungrammatical
errors among all predictions (one prediction may have
multiple errors). Full visualization is in Appendix B.
shown in the output), contradiction (the output is
contradictory to the input), 3) hallucination (the
output contains information that cannot be veriﬁed
by the input), and 4) ungrammatical . Figure 3
shows that the proportion of ungrammatical outputs
is generally less than 5%. Missing information and
contradiction are common errors made by T5, and
performance gains generally come from reducing
contradiction. Hallucination is not a common error
made by T5 except for the highlighted-table-to-text
task (ToTTo), where T5 tends to output information
of non-highlighted cell values.
Case study We summarize some interesting ob-
servations about the model output (more in Ap-
pendix H). Compared with T5-base and T5-large,
T5-3B’s outputs for text generation tasks tend to be
more diverse and creative as shown in Appendix
H.2 and H.7. Also, T5-3B sometimes leverages do-
main knowledge to summarize facts in some tasks
such as DART (e.g., describing rating 5 out of 5 as
low), while the other two copy the original expres-
sions in the input, as shown in Appendix H.5 and
H.6. However, this ability puts T5-3B in the risk
of manipulating information and meaning of user
request as shown in Appendix H.3.2 and H.4.
5 Conclusions
In this paper, we propose the UNIFIED SKG frame-
work to promote systematic research on struc-tured knowledge grounding by unifying 21 SKG
tasks. Using UNIFIED SKG as a benchmark, we
demonstrate that ﬁnetuning T5 on individual tasks
achieves state-of-the-art results on almost all 21
tasks. We show that multi-task preﬁx-tuning bene-
ﬁts most SKG tasks, largely improving the overall
performance. For structured knowledge encoding,
we ﬁnd that the effectiveness of encoding varia-
tions varies across tasks. Moreover, UNIFIED SKG
is a challenging testbed for zero-shot and few-shot
learning, shown by the poor results of large PLMs.
6 Limitations
UNIFIED SKG establishes a powerful and repro-
ducible starting point for SKG research. New mod-
els can be easily applied to diverse SKG tasks, and
new tasks can be easily framed based on our stan-
dardized abstraction. UNIFIED SKG promotes a
systematic study on more general and robust ad-
vances in structured knowledge encoding, multi-
task learning, zero-shot learning, and few-shot
learning for SKG tasks. It also would be interest-
ing to explore general pretraining methods within
UNIFIED SKG, which potentially beneﬁt all the uni-
ﬁed tasks. When the structured knowledge is too
large for GPU memory, we truncate them based on
heuristic rules, calling for future study on 1) incor-
porating retrieval component in SKG, 2) designing
sparse attention in T5 for structured knowledge or
other means to improve model efﬁciency.
UNIFIED SKG currently provides the correct type
of structured knowledge for each task. However,
how a system searches for the correct structured
knowledge resources, takes appropriate action, and
integrates information and results from multiple
structured sources given a user request is still under-
explored, which are a prerequisite for building a
uniﬁed multi-purpose SKG system.
Since we select popular tasks from each task
family, we risk disproportionality in terms of the
data language, domain and population, and we ac-
tively welcome diverse, multi-lingual tasks to be
added into UNIFIED SKG. Also, the error analysis
of SKG can more ﬁne-grained, and we hope our
ﬁndings can promote future work on systematically
studying and decomposing the behavior of PLMs
on SKG tasks. Furthermore, training and evalua-
tion data should reﬂect the intents and linguistic
phenomena in the real world (de Vries et al., 2020),
suggesting more realistic tasks to be added into
UNIFIED SKG.References
Oshin Agarwal, Heming Ge, Siamak Shakeri, and
Rami Al-Rfou. 2020. Knowledge graph based syn-
thetic corpus generation for knowledge-enhanced
language model pre-training. arXiv preprint
arXiv:2010.12688 .
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava,
Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.
2021a. Muppet: Massive multi-task representa-
tions with pre-ﬁnetuning. In Proceedings of EMNLP
2021 , pages 5799–5811, Online and Punta Cana, Do-
minican Republic.
Armen Aghajanyan, Dmytro Okhonko, Mike Lewis,
Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-
moyer. 2021b. Htlm: Hyper-text pre-training and
prompting of language models.
Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, Oana Cocarascu, and Arpit
Mittal. 2021. The fact extraction and VERiﬁca-
tion over unstructured and structured information
(FEVEROUS) shared task. In Proceedings of
the Fourth Workshop on Fact Extraction and
VERiﬁcation (FEVER) , pages 1–13.
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,
Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-
glei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,
Jai Gupta, Kai Hui, Sebastian Ruder, and Donald
Metzler. 2021. Ext5: Towards extreme multi-task
scaling for transfer learning.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc V . Le, and
Charles Sutton. 2021. Program synthesis with large
language models. ArXiv , abs/2108.07732.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP 2013 , pages
1533–1544.
Jonathan Berant and Percy Liang. 2014. Semantic pars-
ing via paraphrasing. In Proceedings of ACL 2014 ,
pages 1415–1425.
Ben Bogin, Matt Gardner, and Jonathan Berant. 2019.
Global reasoning over database structures for text-to-
sql parsing. In Proceedings of EMNLP 2019 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
V oss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language Models are Few-Shot Learners. InAdvances in Neural Information Processing Systems ,
volume 33, pages 1877–1901.
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang
Tseng, Iñigo Casanueva, Ultes Stefan, Ramadan Os-
man, and Milica Gaši ´c. 2018. Multiwoz - a large-
scale multi-domain wizard-of-oz dataset for task-
oriented dialogue modelling. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing (EMNLP) .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021a. Eval-
uating large language models trained on code.
Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,
Ruofei Zhang, and Yangfeng Ji. 2021b. HittER: Hi-
erarchical transformers for knowledge graph embed-
dings. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 10395–10407.
Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and
William Yang Wang. 2020a. Logical natural lan-
guage generation from open-domain tables. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 7929–
7942, Online. Association for Computational Lin-
guistics.
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020b. Tabfact : A large-scale
dataset for table-based fact veriﬁcation. In Inter-
national Conference on Learning Representations
(ICLR) , Addis Ababa, Ethiopia.
Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan
Xiong, Hong Wang, and William Wang. 2020c. Hy-
bridqa: A dataset of multi-hop question answering
over tabular and textual data. Findings of EMNLP
2020 .
Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou
Zhou, Yunkai Zhang, Sairam Sundaresan, and
William Yang Wang. 2020d. Logic2Text: High-
ﬁdelity natural language generation from logical
forms. In Findings of the Association for Compu-
tational Linguistics: EMNLP 2020 .Zimin Chen, Vincent Josua Hellendoorn, Pascal Lam-
blin, Petros Maniatis, Pierre-Antoine Manzagol,
Daniel Tarlow, and Subhodeep Moitra. 2021c.
PLUR: A unifying, graph-based view of program
learning, understanding, and repair. In Thirty-Fifth
Conference on Neural Information Processing Sys-
tems.
Jordan Clive, Kris Cao, and Marek Rei. 2021. Control
preﬁxes for parameter-efﬁcient text generation.
Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei
Huang, Luo Si, and Xiaodan Zhu. 2021. Preview,
attend and review: Schema-aware curriculum learn-
ing for multi-domain dialogue state tracking. In Pro-
ceedings ACL-IJCNLP 2021 (Volume 2: Short Pa-
pers) , pages 879–885, Online.
Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya
Godbole, Ethan Perez, Jay Yoon Lee, Lizhen
Tan, Lazaros Polymenakos, and Andrew McCallum.
2021. Case-based reasoning for natural language
queries over knowledge bases. In Proceedings of
EMNLP 2021 , pages 9594–9611, Online and Punta
Cana, Dominican Republic.
Harm de Vries, Dzmitry Bahdanau, and Christopher D.
Manning. 2020. Towards ecologically valid re-
search on language user interfaces. ArXiv .
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju,
Rishabh Singh, Abdel rahman Mohamed, and Push-
meet Kohli. 2017. Robustﬁll: Neural program learn-
ing under noisy i/o. In ICML .
Julian Martin Eisenschlos, Maharshi Gor, Thomas
Müller, and William W Cohen. 2021. Mate: Multi-
view attention for table transformer efﬁciency. arXiv
preprint arXiv:2109.04312 .
Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,
Sanchit Agarwal, Shuyag Gao, and Dilek Hakkani-
Tur. 2019. Multiwoz 2.1: Multi-domain dialogue
state corrections and state tracking baselines. arXiv
preprint arXiv:1907.01669 .
Mihail Eric, Lakshmi. Krishnan, François Charette,
and Christopher D. Manning. 2017. Key-value re-
trieval networks for task-oriented dialogue. In SIG-
DIAL Conference .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Association for Computational Linguis-
tics (ACL) .
Yanjie Gou, Yinjie Lei, Lingqiao Liu, Yong Dai,
and Chunxu Shen. 2021. Contextualize knowledge
bases with transformer for end-to-end task-oriented
dialogue systems. In Proceedings of the EMNLP
2021 , pages 4300–4310, Online and Punta Cana, Do-
minican Republic.
Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy
Liang, Xifeng Yan, and Yu Su. 2021. Beyond iid:
three levels of generalization for question answeringon knowledge bases. In Proceedings of the Web Con-
ference 2021 .
Jonathan Herzig, P. Nowak, Thomas Müller, Francesco
Piccinno, and Julian Martin Eisenschlos. 2020.
Tapas: Weakly supervised table parsing via pre-
training. In Proceedings of ACL .
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,
Semih Yavuz, and Richard Socher. 2020. A simple
language model for task-oriented dialogue. In Pro-
ceedings of Conference on Neural Information Pro-
cessing Systems (NeurIPS) .
Binyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li,
Yongbin Li, Jian Sun, Fei Huang, Luo Si, Pengfei
Zhu, and Xiaodan Zhu. 2021. Dynamic hybrid re-
lation network for cross-domain context-dependent
semantic parsing.
Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and
Minjoon Seo. 2019. A comprehensive exploration
on wikisql with table-aware word contextualization.
ArXiv , abs/1902.01069.
Hiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit
Iyyer. 2021. Tabbie: Pretrained representations of
tabular data. arXiv preprint arXiv:2105.02584 .
Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , pages
1821–1831, Vancouver, Canada.
Mihir Kale and Abhinav Rastogi. 2020. Text-to-text
pre-training for data-to-text tasks. In Proceedings of
INLG 2020, Dublin, Ireland, December 15-18, 2020 ,
pages 97–102.
D. Khashabi, S. Min, T. Khot, A. Sabhwaral,
O. Tafjord, P. Clark, and H. Hajishirzi. 2020. Uni-
ﬁedqa: Crossing format boundaries with a single qa
system. EMNLP - ﬁndings .
Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf.
2021. Dialogue state tracking with a language
model using schema-driven prompting. In Proceed-
ings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 4937–4949.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. In Proceedings of EMNLP 2021, Virtual
Event / Punta Cana, Dominican Republic, 7-11
November, 2021 , pages 3045–3059.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,Sylvain Gugger, Clément Delangue, Théo Matus-
sière, Lysandre Debut, Stas Bekman, Pierric Cistac,
Thibault Goehringer, Victor Mustar, François Lagu-
nas, Alexander M. Rush, and Thomas Wolf. 2021.
Datasets: A community library for natural language
processing.
Haoran Li, Abhinav Arora, Shuohui Chen, Anchit
Gupta, Sonal Gupta, and Yashar Mehdad. 2021.
MTOP: A comprehensive multilingual task-oriented
semantic parsing benchmark. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 2950–2962, Online.
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
4582–4597, Online.
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang
Ren. 2019. Kagnet: Knowledge-aware graph net-
works for commonsense reasoning. In Proceedings
of EMNLP-IJCNLP .
Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A
Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu,
Andrea Madotto, Eunjoon Cho, and Rajen Subba.
2021. Leveraging slot descriptions for zero-shot
cross-domain dialogue statetracking. In Proceed-
ings of NAACL 2021 , pages 5640–5648.
Qian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and Jian
guang Lou. 2021. Tapex: Table pre-training via
learning a neural sql executor.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,
Haotang Deng, and Ping Wang. 2020. K-bert:
Enabling language representation with knowledge
graph. In AAAI .
Andrea Madotto, Chien-Sheng Wu, and Pascale Fung.
2018. Mem2seq: Effectively incorporating knowl-
edge bases into end-to-end task-oriented dialog sys-
tems. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1468–1478.
Alana Marzoev, Samuel Madden, M Frans Kaashoek,
Michael Cafarella, and Jacob Andreas. 2020. Unnat-
ural language processing: Bridging the gap between
synthetic and natural language data. arXiv preprint
arXiv:2004.13645 .
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,
and Richard Socher. 2018. The natural language de-
cathlon: Multitask learning as question answering.
CoRR , abs/1806.08730.
Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Vic-
toria Lin, Neha Verma, Rui Zhang, Wojciech Kry ´s-
ci´nski, Nick Schoelkopf, Riley Kong, Xiangru Tang,
Murori Mutuma, Ben Rosand, Isabel Trindade,Renusree Bandaru, Jacob Cunningham, Caiming
Xiong, and Dragomir Radev. 2021a. Fetaqa: Free-
form table question answering. TACL .
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit
Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xian-
gru Tang, Aadit Vyas, Neha Verma, Pranav Kr-
ishna, Yangxiaokang Liu, Nadia Irwanto, Jessica
Pan, Faiaz Rahman, Ahmad Zaidi, Murori Mutuma,
Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan,
Xi Victoria Lin, Caiming Xiong, Richard Socher,
and Nazneen Fatema Rajani. 2021b. Dart: Open-
domain structured data record to text generation. In
NAACL .
Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.
2017. The E2E dataset: New challenges for end-to-
end generation. In SIGDial 2017 , pages 201–206.
Barlas Oguz, Xilun Chen, Vladimir Karpukhin,
Stan Peshterliev, Dmytro Okhonko, Michael
Schlichtkrull, Sonal Gupta, Yashar Mehdad, and
Scott Yih. 2021. Unik-qa: Uniﬁed representations
of structured and unstructured knowledge for
open-domain question answering. arXiv preprint
arXiv:2012.14610 .
Ankur P Parikh, Xuezhi Wang, Sebastian Gehrmann,
Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and
Dipanjan Das. 2020. ToTTo: A controlled table-to-
text generation dataset. In Proceedings of EMNLP .
Panupong Pasupat and Percy Liang. 2015. Composi-
tional semantic parsing on semi-structured tables. In
Proceedings of the 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 1470–
1480, Beijing, China.
Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.
Controllable semantic parsing via retrieval augmen-
tation. In Proceedings of EMNLP 2021 , pages 7683–
7698, Online and Punta Cana, Dominican Republic.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers , pages 186–
191, Belgium, Brussels.
Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, and
Ting Liu. 2020. Dynamic fusion network for multi-
domain end-to-end task-oriented dialog. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 6344–
6354, Online.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the lim-
its of transfer learning with a uniﬁed text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.Nils Reimers and Iryna Gurevych. 2020. Making
monolingual sentence embeddings multilingual us-
ing knowledge distillation. In Proceedings of
EMNLP 2020 .
Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen,
Michihiro Yasunaga, Haitian Sun, Dale Schuurmans,
Jure Leskovec, and Denny Zhou. 2021. Lego: La-
tent execution-guided reasoning for multi-hop ques-
tion answering on knowledge graphs. In Interna-
tional Conference on Machine Learning (ICML) .
Ohad Rubin and Jonathan Berant. 2021. SmBoP: Semi-
autoregressive bottom-up semantic parsing. In Pro-
ceedings of NAACL 2021 , pages 311–324, Online.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,
Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Thakker, Shanya Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak,
Debajyoti Datta, Jonathan Chang, Mike Tian-Jian
Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-
heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-
son Alan Fries, Ryan Teehan, Stella Biderman, Leo
Gao, Tali Bers, Thomas Wolf, and Alexander M.
Rush. 2021. Multitask prompted training enables
zero-shot task generalization.
Apoorv Saxena, Aditay Tripathi, and Partha Taluk-
dar. 2020. Improving multi-hop question answer-
ing over knowledge graphs using knowledge base
embeddings. In Association for Computational Lin-
guistics (ACL) .
Torsten Scholak, Nathan Schucher, and Dzmitry Bah-
danau. 2021. PICARD: Parsing incrementally for
constrained auto-regressive decoding from language
models. In Proceedings of EMNLP 2021 , pages
9895–9901.
Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and
Kristina Toutanova. 2021. Compositional general-
ization and natural language variation: Can a seman-
tic parsing approach handle both? In ACL/IJCNLP .
Richard Shin, Christopher H Lin, Sam Thomson,
Charles Chen, Subhro Roy, Emmanouil Antonios
Platanios, Adam Pauls, Dan Klein, Jason Eisner, and
Benjamin Van Durme. 2021. Constrained language
models yield few-shot semantic parsers. arXiv
preprint arXiv:2104.08768 .
Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi,
Tao Yu, and Rui Zhang. 2021. Logic-consistency
text generation from semantic parses. In Findings of
the Association for Computational Linguistics: ACL-
IJCNLP 2021 .
Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang,
and Nigel Collier. 2021. Plan-then-generate: Con-
trolled data-to-text generation via planning. In
EMNLP .A. Talmor and J. Berant. 2018. The web as a
knowledge-base for answering complex questions.
InNorth American Association for Computational
Linguistics (NAACL) .
Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,
Yizhong Wang, Akari Asai, Gabriel Ilharco, Han-
naneh Hajishirzi, and Jonathan Berant. 2021. Mul-
timodal{qa}: complex question answering over text,
tables and images. In International Conference on
Learning Representations .
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou,
and Daniel Cer. 2021. Spot: Better frozen model
adaptation through soft prompt transfer.
Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr
Polozov, and Matthew Richardson. 2020. Rat-sql:
Relation-aware schema encoding and linking for
text-to-sql parsers. In ACL.
Bailin Wang, Ivan Titov, and Mirella Lapata. 2019.
Learning semantic parsers from denotations with la-
tent structured alignments and abstract programs. In
Proceedings of EMNLP-IJCNLP 2019 , pages 3774–
3785, Hong Kong, China.
Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
2017. Knowledge graph embedding: A survey of
approaches and applications. IEEE Transactions
on Knowledge and Data Engineering , 29(12):2724–
2743.
Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,
and Hao Ma. 2021a. Entailment as few-shot learner.
CoRR , abs/2104.14690.
Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu,
Shi Han, and Dongmei Zhang. 2021b. Tuta: Tree-
based transformers for generally structured table pre-
training. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Min-
ing, pages 1780–1790.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint .
Jason D Williams, Antoine Raux, and Matthew Hen-
derson. 2016. The dialog state tracking challenge
series: A review. Dialogue & Discourse , 7(3):4–33.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of EMNLP 2020: System
Demonstrations , pages 38–45, Online.Chien-Sheng Wu, Richard Socher, and Caiming Xiong.
2019. Global-to-local memory pointer networks for
task-oriented dialogue. In Proceedings of the Inter-
national Conference on Learning Representations
(ICLR) .
Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhi-
gang Chen, and Xiaodan Zhu. 2020. Program en-
hanced fact veriﬁcation with verbalization and graph
attention network. In Proceedings of EMNLP 2020 ,
pages 7810–7825, Online.
Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren,
Xikun Zhang, Christopher D. Manning, Percy Liang,
and Jure Leskovec. 2022. Deep bidirectional
language-knowledge graph pretraining. In Neural
Information Processing Systems (NeurIPS) .
Michihiro Yasunaga and Percy Liang. 2020. Graph-
based, self-supervised program repair from diagnos-
tic feedback. In International Conference on Ma-
chine Learning (ICML) .
Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut,
Percy Liang, and Jure Leskovec. 2021. QA-GNN:
Reasoning with language models and knowledge
graphs for question answering. In North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL) .
Association for Computational Linguistics.
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021a.
Crossﬁt: A few-shot learning challenge for cross-
task generalization in nlp. In Proceedings of
EMNLP .
Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo
Zhou, and Caiming Xiong. 2021b. Rng-kbqa:
Generation augmented iterative ranking for knowl-
edge base question answering. arXiv preprint
arXiv:2109.08678 .
Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-
Wei Chang, and Jina Suh. 2016. The value of se-
mantic parse labeling for knowledge base question
answering. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) .
Pengcheng Yin and Graham Neubig. 2018. TRANX:
A transition-based neural abstract syntax parser for
semantic parsing and code generation. In EMNLP
2018: System Demonstrations , pages 7–12.
Pengcheng Yin, Graham Neubig, Wen-tau Yih, and
Sebastian Riedel. 2020a. TaBERT: Pretraining for
joint understanding of textual and tabular data. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics . Association
for Computational Linguistics.
Wenpeng Yin, Nazneen Fatema Rajani, Dragomir R.
Radev, Richard Socher, and Caiming Xiong. 2020b.
Universal natural language processing with limited
annotations: Try few-shot textual entailment as a
start. In Proceedings of EMNLP 2020, Online,
November 16-20, 2020 , pages 8229–8239.Ori Yoran, Alon Talmor, and Jonathan Berant.
2021. Turning tables: Generating examples
from semi-structured tables for endowing language
models with reasoning skills. arXiv preprint
arXiv:2107.07261 .
Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,
Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze
Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,
Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan
Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-
cent Zhang, Caiming Xiong, Richard Socher, Walter
Lasecki, and Dragomir Radev. 2019a. CoSQL: A
conversational text-to-SQL challenge towards cross-
domain natural language interfaces to databases. In
Proceedings of EMNLP 2019 , pages 1962–1979,
Hong Kong, China.
Tao Yu, Rui Zhang, Oleksandr Polozov, Christo-
pher Meek, and Ahmed Hassan Awadallah. 2021.
SCoRE: Pre-training for context representation in
conversational semantic parsing. In International
Conference on Learning Representations .
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018. Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-sql task. In
Proceedings of EMNLP 2018 , Brussels, Belgium.
Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern
Tan, Xi Victoria Lin, Suyi Li, Irene Li Heyang Er,
Bo Pang, Tao Chen, Emily Ji, Shreya Dixit,
David Proctor, Sungrok Shim, Vincent Zhang
Jonathan Kraft, Caiming Xiong, Richard Socher,
and Dragomir Radev. 2019b. Sparc: Cross-domain
semantic parsing in context. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics , Florence, Italy.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, and Amr Ahmed. 2020. Big Bird: Trans-
formers for Longer Sequences. In Advances in
Neural Information Processing Systems , volume 33,
pages 17283–17297.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI 1996 , pages 1050–1055.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classiﬁcation with probabilistic categorial grammars.
UAI.
Hongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi
Cao, Fuzheng Zhang, and Zhongyuan Wang. 2020.
Table fact veriﬁcation with structure-aware trans-
former. In Proceedings of EMNLP 2020 , pages
1624–1629.Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.
2021. Adapting language models for zero-shot
learning by meta-tuning on dataset and prompt col-
lections. In Findings of EMNLP .
Ruiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic
evaluation for text-to-sql with distilled test suite. In
EMNLP 2020 .
Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries
from natural language using reinforcement learning.
CoRR , abs/1709.00103.A Contributions
Code implementation Tianbao Xie and Chen
Henry Wu implemented the code base of the UNI-
FIED SKG framework and experiment pipeline. The
code of PICARD and advice from Torsten Scholak
sped up the implementation.
Task uniﬁcation Tianbao Xie, Peng Shi, Michi-
hiro Yasunaga, Chen Henry Wu, and Ming Zhong
implemented the 21 tasks into the text-to-text for-
mat, adapted the metrics, and veriﬁed the perfor-
mances.
Paper writing Chen Henry Wu and Tianbao
Xie ﬁnished most part of the paper. Michihiro
Yasunaga, Peng Shi, and Chengzu Li added re-
sults and analysis for their corresponding parts.
Peng Shi drafted related work on SKG with PLMs.
Torsten Scholak, Pengcheng Yin, Rui Zhang, Ruiqi
Zhong, Victor Zhong, Michihiro Yasunaga, Connor
Boyle, Chien-Sheng Wu, Sida Wang, Bailin Wang,
Ansong Ni, Ziyu Yao, Lingpeng Kong, Caiming
Xiong, Dragomir Radev, Noah A. Smith, and Luke
Zettlemoyer carefully reviewed the paper and gave
feedback for multiple rounds.
Experiments Chen Henry Wu, Tianbao Xie,
and Chien-Sheng Wu conducted experiments on
individual tasks and multi-task learning. Tian-
bao conducted the zero-shot learning experiments.
Chengzu Li and Tianbao Xie conducted the few-
shot learning experiments. Tianbao Xie conducted
experiments on the ordering of sequence inputs and
order-sensitivity. Chengzu Li, Connor Boyle, and
Peng Shi conducted the experiments on converting
structured knowledge into natural language.
Human evaluation Chen Henry Wu organized
the human evaluation. Torsten Scholak, Rui Zhang,
Chengzu Li, Connor Boyle, Tianbao Xie, Peng
Shi, Tao Yu, and Chen Henry Wu were the human
participants.
Error analysis and case study Tianbao Xie,
Chen Henry Wu, and Michihiro Yasunaga designed
and conducted the error analysis for semantic pars-
ing and generation tasks. Authors who participated
in the human annotation selected the cases for case
study.
Discussion We had three separate weekly meet-
ings, and everyone in the project attended one of
them. Torsten Scholak, Ruiqi Zhong, Pengcheng
Yin, Victor Zhong, Peng Shi, Rui Zhang, Sida
Wang, and Lingpeng Kong actively provided ad-
vice. Torsten Scholak provided signals that preﬁx-
tuning would be comparable to ﬁne-tuning. RuiqiZhong gave advice on analyzing the effect of model
size, Pengcheng Yin and Peng Shi gave advice on
analysis on converting structured knowledge into
natural language. Pengcheng Yin helped interpret
experimental results. Ziyu Yao suggested that we
report both sota (w/ extra) and sota (w/o extra) for
a fair comparison. Victor Zhong and Bailin Wang
gave valuable suggestions on multi-task learning
and task transfer analysis. Luke Zettlemoyer, Noah
A. Smith, Caiming Xiong, and Dragomir Radev
gave valuable comments on research questions and
experimental design.
Computing resources We thank Salesforce Re-
search, an Amazon Research Award, ServiceNow
Research, and Yale NLP for providing computing
resources generously.
Tao Yu designed and led the research.
Acknowledgments
We thank Yifei Min and Libo Qin for their early-
stage discussion. We thank Panupong Pasupat and
William W. Cohen for their valuable feedback on
our initial draft. We thank Qian Liu for his TAPEX
code and advice on question answering tasks. We
thank wandb for free logging and OpenAI for free
Codex usage.B Results with Full Metrics
Metric T5-base T5-large T5-3B
SpiderMatch 58.12 1.46 66.63 2.31 71.76
Exec 60.06 0.54 68.28 1.61 74.37
Test suite 56.22 0.73 64.12 1.28 68.38
GrailQA Match 60.00 67.00 69.00
WebQSP F1 72.50 73.96 75.97
MTOPMatch 83.89 84.70 84.88
Template 88.85 88.32 88.86
WikiTQ Acc 36.94 0.19 43.30 0.25 50.65
WikiSQL Acc 84.50 86.27 87.34
CompWebQAcc 66.71 68.85 70.27
F1 80.02 81.05 81.43
Hits@1 83.64 85.49 86.20
HybridQAAcc 54.07 56.95 59.41
F1 61.85 64.62 66.76
MMQAAcc 67.29 74.08 78.48
F1 75.51 81.84 82.28
FeTaQA BLEU 29.00 30.94 31.73
DART BLEU 50.62 0.72 51.72 0.15 50.38
ToTTo BLEU 48.29 48.95 48.95
MultiWoZ2.1 Joint Acc 57.52 0.96 58.23 0.39 58.46
KVRET BLEU 20.04 18.84 17.75
SParCMatch 50.54 56.69 61.51
Exec 53.95 60.60 67.33
Match (interact) 31.28 37.44 41.94
Exec (interact) 34.36 41.23 46.45
CoSQLMatch 42.30 48.26 54.08
Exec 49.26 56.01 62.23
Match (interact) 12.63 16.72 22.78
Exec (interact) 16.04 20.14 26.16
SQA Overall Acc 49.49 59.12 60.93
TabFact Acc 76.34 0.36 81.40 0.16 83.97
FEVEROUS Acc 75.05 79.81 82.40
SQL2Text BLEC 93.69 0.29 93.35 0.29 92.71
Logic2Text BLEC 92.15 92.88 91.69
Table 11: Development set performance with full met-
rics. We do three experiments with different random
seeds on representative task of each family and report
their averages and standard variances format as avr var.
For the KVRET dataset, instead of the version
used in our main tables, we re-run another more
widely used pre-processed version (Madotto et al.,
2018; Wu et al., 2019; Qin et al., 2020) on T5-base,
T5-large and T5-3b. Results are shown in Table 13.
C Input and Output Length Analysis
Linearization of large structured knowledge input
(e.g., large tables and KGs) can be arbitrarily long,
which needs to be truncated to ﬁt in GPUs with aMetric T5-base T5-large T5-3B
GrailQA Match 62.39 67.30 70.11
WebQSP F1 78.83 79.45 80.70
MTOPMatch 85.49 86.17 86.78
Template 87.52 89.53 90.20
WikiTQ Acc 35.76 0.66 43.22 0.65 49.29
WikiSQL Acc 82.63 84.80 85.96
CompWebQAcc 68.43 71.38 73.26
F1 80.20 81.76 82.58
Hits@1 83.70 85.40 86.08
FeTaQABLEU 29.91 32.45 33.44
ROUGE-1-Fmeasure 61.77 64.01 65.21
ROUGE-2-Fmeasure 39.44 42.26 43.09
ROUGE-L-Fmeasure 51.93 54.29 55.31
METEOR 48.53 50.80 51.23
BertScore-F1 0.92 0.93 0.93
BLEURT -0.01 0.06 0.09
DARTBLEU 46.22 0.66 46.89 0.53 46.66
TER 61.80 0.20 60.97 0.31 60.70
METEOR 55.09 0.35 55.76 0.25 55.67
BertScore-F1 0.95 0.00 0.95 0.00 0.95
BLEURT 0.2833 0.0057 0.30 0.00 0.30
MultiWoZ2.1 Joint Acc 54.64 0.22 54.45 0.20 55.42
KVRETBLEU 17.41 17.27 15.45
F1 micro all 66.45 65.85 67.88
F1 micro schedule 73.48 75.90 77.99
F1 micro navigate 64.89 62.72 65.47
F1 micro weather 63.78 62.80 64.01
SQAOverall Acc 52.91 61.28 62.37
Pos 0 Acc 62.93 67.80 59.51
Pos 1 Acc 44.43 55.08 60.25
Pos 2 Acc 50.44 61.88 68.77
Pos 3 Acc 53.71 58.08 65.07
Interaction Acc 22.24 32.59 33.17
TabFactAll Acc 76.13 0.39 80.85 0.24 83.68
Simple Acc - 91.38 0.32 93.10
Complex Acc - 75.76 0.19 79.12
Small Acc - 82.61 0.32 85.39
SQL2Text BLEC 93.52 1.00 93.68 1.12 94.78
Logic2Text BLEC 90.66 90.57 91.39
Table 12: Test set performance with full metrics (for
tasks with a publicly available test set). We do three
experiments with different random seeds on representa-
tive task of each family and report their averages and
standard variances format as avr var.
limited size. The input and output are tokenized
by T5Tokenizer in Huggingface’s Transformers.3
We visualize the length distribution in Figure 5,
and details are presented in Table 14. Among the
datasets with very long inputs, we choose Wik-
iTableQuestion to study the impact of input length.
We visualize the table length distribution and per-
formances with different input truncation lengths in
Figure 6. We observe that the accuracy increases as
the input becomes longer, motivating future work to
study how to effectively encode large structured in-
put, e.g., leveraging sparse attention (Zaheer et al.,
2020).
3https://huggingface.co/t5-base/tree/mainbase large 3b
Size0100200300CountError type
Invalid Valid-but-wrong(a) Spider
base large 3b
Size0100200300CountError type
Invalid Valid-but-wrong (b) CoSQL
base large 3b
Size0100200300CountError type
Invalid Valid-but-wrong (c) SParC
base large 3b
Size050100CountError type
Invalid Valid-but-wrong (d) GrailQA
base large 3b
Size050100150CountError type
Invalid Valid-but-wrong
(e) WebQSP
base large 3b
Size0100200300400CountError type
Invalid Valid-but-wrong (f) MTOP
base large 3b
Size01020304050ProportionError type
Miss. Inf.
Contra.Halluc.
Ungram. (g) FeTaQA
base large 3b
Size05101520ProportionError type
Miss. Inf.
Contra.Halluc.
Ungram. (h) DART
base large 3b
Size0102030ProportionError type
Miss. Inf.
Contra.Halluc.
Ungram.
(i) ToTTo
base large 3b
Size0102030ProportionError type
Miss. Inf.
Contra.Halluc.
Ungram. (j) KVRET
base large 3b
Size05101520ProportionError type
Miss. Inf.
Contra.Halluc.
Ungram. (k) SQL2Text
base large 3b
Size01020ProportionError type
Miss. Inf.
Contra.Halluc.
Ungram. (l) Logic2Text
Figure 4: Error analysis. For semantic parsing, we show the number of invalid/valid-but-wrong predictions.
For generation tasks, we show the proportion of missing-information/contradiction/hallucination/ungrammatical
predictions among all predictions (one prediction may have multiple errors).
Metric T5-base T5-large T5-3B
BLEU(dev) 22.80 23.07 22.71
BLEU(test) 21.21 22.36 20.40
F1 micro all(test) 67.49 68.03 70.07
F1 micro schedule(test) 79.39 79.47 78.54
F1 micro navigate(test) 62.87 63.59 65.34
F1 micro weather(test) 61.43 62.61 66.74
F1 macro all(test) 65.91 64.87 66.07
F1 macro schedule(test) 78.73 77.23 76.02
F1 macro navigate(test) 59.53 58.99 60.47
F1 macro weather(test) 64.05 62.58 65.78
Table 13: Baselines results are higher in pre-processed
KVRET dataset. It doesn’t change our conclusion on
T5 with simple modiﬁcation when necessary achieves
sota on almost all tasks.
D Experimental Setup
D.1 Implementation Details
We use T5 (Raffel et al., 2020) as our backbone
language model. Each experiment For T5-3B ex-
periments, we use Deepspeed4to save memory.
We use batch size 32 as default, except WikiTQ,
4https://github.com/microsoft/DeepSpeed
/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013
/uni0000004c/uni00000051/uni00000053/uni00000058/uni00000057/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni0000000b/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni0000004c/uni00000051/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000e/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000056/uni00000057/uni00000003/uni0000004c/uni00000051/uni00000053/uni00000058/uni00000057/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000018/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000027/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c/uni00000046/uni00000052/uni00000050/uni00000053/uni0000005a/uni00000048/uni00000045/uni00000054
/uni00000046/uni00000052/uni00000056/uni00000054/uni0000004f
/uni00000047/uni00000044/uni00000055/uni00000057
/uni00000049/uni00000048/uni00000057/uni00000044/uni00000054/uni00000044
/uni00000049/uni00000048/uni00000059/uni00000048/uni00000055/uni00000052/uni00000058/uni00000056
/uni0000004a/uni00000055/uni00000044/uni0000004c/uni0000004f/uni00000054/uni00000044
/uni0000004b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047/uni00000054/uni00000044
/uni0000004e/uni00000059/uni00000055/uni00000048/uni00000057
/uni0000004f/uni00000052/uni0000004a/uni0000004c/uni00000046/uni00000015/uni00000057/uni00000048/uni0000005b/uni00000057
/uni00000050/uni00000050/uni00000054/uni00000044
/uni00000050/uni00000058/uni0000004f/uni00000057/uni0000004c/uni0000005a/uni00000052/uni0000005d
/uni00000052/uni00000057/uni00000057/uni00000054/uni00000044
/uni00000056/uni00000053/uni00000044/uni00000055/uni00000046
/uni00000056/uni00000053/uni0000004c/uni00000047/uni00000048/uni00000055
/uni00000056/uni00000054/uni00000044
/uni00000056/uni00000054/uni0000004f/uni00000015/uni00000057/uni00000048/uni0000005b/uni00000057
/uni00000057/uni00000044/uni00000045/uni00000042/uni00000049/uni00000044/uni00000046/uni00000057
/uni00000057/uni00000052/uni00000057/uni00000057/uni00000052
/uni0000005a/uni00000048/uni00000045/uni00000054/uni00000056/uni00000053
/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000056/uni00000054/uni0000004f
/uni0000005a/uni0000004c/uni0000004e/uni0000004c/uni00000057/uni00000054Figure 5: Input token distribution(<4096) in train set
from different tasks. We exclude MTOP since it con-
centrates on a relatively small ﬁeld which would make
this ﬁgure unreadable. In general, 1024 is a good length
for practice, and for most tasks, 2048 can hold all its in-
puts.
WikiSQL, and TabFact, for which for use batch
size 128 because we found it to work signiﬁcantly
better. We use the Adafactor optimizer for T5-base
and T5-large, and AdamW for T5-3b. We evaluate
on the development set for each 500 steps and use
the average development set metric for best check-Structure Input Tokens Text Input Tokens Structure Input + Text Input Tokens Sequence Output Tokens
Distribution(%) [0, 512) [512, 1024) [1024,
8) [0, 512) [512, 1024) [1024,
8) [0, 512) [512, 1024) [1024,
8) [0, 128) [128, 256) [256,
8)
Spider 97.01 1.81 1.17 100.00 0.00 0.00 95.47 3.35 1.17 98.81 1.18 0.0
GRAILQA 100.00 0.00 0.00 100.00 0.00 0.00 99.96 0.04 0.00 99.97 0.03 0.00
WebQsp 3.40 2.32 94.28 100.00 0.00 0.00 3.18 2.47 94.35 99.81 0.19 0.00
MTOP 0.00 100.00 0.00 100.00 0.00 0.00 0.00 100.00 0.00 99.97 0.03 0.00
WikiTableQuestions 48.32 27.48 24.18 100.00 0.00 0.00 46.03 29.43 24.52 99.98 0.01 0.01
WikiSQL 63.38 25.33 11.29 100.00 0.00 0.00 61.50 26.79 11.70 99.97 0.02 0.01
ComWebQ 1.18 14.52 84.30 100.00 0.00 0.00 1.09 11.28 87.63 99.59 0.39 0.01
HybridQA 35.53 50.63 13.8 100.00 0.00 0.00 31.77 53.35 14.86 100.00 0.00 0.0
MultiModalQA 63.02 25.67 11.30 100.00 0.00 0.00 60.54 27.26 12.18 99.99 0.01 0.00
FeTaQA 60.36 28.62 11.01 100.00 0.00 0.00 58.46 29.85 11.68 100.00 0.00 0.0
DART 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 99.99 0.01 0.0
ToTTo 95.80 2.87 1.31 100.00 0.00 0.00 95.80 2.87 1.31 99.99 0.01 0.0
MultiWoZ 100.00 0.00 0.00 98.77 1.21 0.01 54.76 45.09 0.13 0.00 100.00 0.0
KVRET 65.08 34.91 0.00 100.00 0.00 0.00 65.08 34.91 0.00 99.97 0.03 0.0
SParC 96.70 2.02 1.28 100.00 0.00 0.00 95.10 3.62 1.28 99.34 0.66 0.00
CoSQL 96.03 2.23 1.73 100.00 0.00 0.00 93.98 4.28 1.73 99.06 0.93 0.0
SQA 64.54 29.74 5.71 100.00 0.00 0.00 60.96 33.11 5.92 95.12 4.19 0.67
TabFact 63.22 28.19 8.58 100.00 0.00 0.00 60.68 30.20 9.10 100.00 0.00 0.0
FEVEROUS 61.37 22.24 16.39 100.00 0.00 0.00 57.53 25.07 17.40 100.00 0.00 0.00
SQL2Text 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.0 100.00 0.00 0.0
Logic2Text 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.0 100.00 0.00 0.0
Table 14: Input and output length for each task’s train set.
Structure Input Tokens Text Input Tokens Structure Input + Text Input Tokens Sequence Output Tokens
Distribution(%) [0, 512) [512, 1024) [1024,
8) [0, 512) [512, 1024) [1024,
8) [0, 512) [512, 1024) [1024,
8) [0, 128) [128, 256) [256,
8)
Spider 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 99.23 0.77 0.00
GRAILQA 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
WebQsp 3.56 1.29 95.15 100.00 0.00 0.00 3.56 1.29 95.15 99.68 0.32 0.00
Russ 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
MTOP 0.00 100.00 0.00 100.00 0.00 0.00 0.00 100.00 0.00 100.00 0.00 0.00
WikiTableQuestions 49.56 28.65 21.79 100.00 0.00 0.00 48.60 29.11 22.29 99.93 0.07 0.00
WikiSQL 63.90 25.88 10.22 100.00 0.00 0.00 62.06 26.99 10.95 100.00 0.00 0.00
ComWebQ 0.28 15.79 83.93 100.00 0.00 0.00 0.28 12.66 87.06 99.00 1.00 0.00
HybridQA 38.37 52.63 9.00 100.00 0.00 0.00 34.16 56.00 9.84 100.00 0.00 0.00
MultiModalQA 66.22 25.72 8.06 100.00 0.00 0.00 64.02 27.38 8.59 100.00 0.00 0.00
FeTaQA 67.03 27.47 5.49 100.00 0.00 0.00 64.84 29.57 5.59 100.00 0.00 0.00
DART 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
ToTTo 95.82 2.92 1.26 100.00 0.00 0.00 95.82 2.92 1.26 100.00 0.00 0.00
MultiWoZ 100.00 0.00 0.00 99.16 0.84 0.00 25.07 74.68 0.24 0.00 100.00 0.00
KVRET 65.76 34.24 0.00 100.00 0.00 0.00 65.76 34.24 0.00 99.79 0.21 0.00
SParC 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 99.26 0.74 0.00
CoSQL 100.00 0.00 0.00 100.00 0.00 0.00 99.62 0.38 0.00 99.23 0.77 0.00
SQA 60.09 33.38 6.53 100.00 0.00 0.00 56.91 36.42 6.67 94.17 5.39 0.44
TabFact 62.17 29.31 8.52 100.00 0.00 0.00 59.95 30.91 9.14 100.00 0.00 0.00
FEVEROUS 61.56 23.71 14.73 100.00 0.00 0.00 57.57 26.58 15.85 100.00 0.00 0.00
SQL2Text 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
Logic2Text 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
Table 15: Input and output length for each task’s development set.
point selection. For all tasks, we set learning rate
to 5e-5 and used linear learning rate decay. All
experiments are done on NVIDIA Tesla V100 and
NVIDIA Tesla A100.
D.2 Metric Details
For most semantic parsing tasks, we report the ex-
act match accuracy of logical forms, and for task
has test suite (Zhong et al., 2020), we add test suite
metric to represent model’s performance; an ex-
ception is WebQSP, for which we follow previous
work to execute the parses and report the F1 score.
For QA, we report the exact match accuracy of
answer sets. For data-to-text generation, we re-port sacre-BLEU (Post, 2018).5We use each task’s
representative metric used by previous works. For
fact veriﬁcation, we report the accuracy. For high-
ﬁdelity NLG, we report BLEC (Shu et al., 2021),
which is the exact match between keywords in the
formal language and the natural language. Unless
speciﬁed, we use T5-large and report the develop-
ment set performance.
D.3 T0 Zero-shot Experimental Details
For each task in UNIFIED SKG we search Sanh et al.
(2021) for the most similar instructions(if there is
no one for use, we create one follow their writing
5Signature: BLEU + case.lc + numrefs.1 + smooth.exp +
tok.13a + version.1.4.0Structure Input Tokens Text Input Tokens Structure Input + Text Input Tokens Sequence Output Tokens
Distribution(%) [0, 512) [512, 1024) [1024,
8) [0, 512) [512, 1024) [1024,
8) [0, 512) [512, 1024) [1024,
8) [0, 128) [128, 256) [256,
8)
Spider - - - - - - - - - - - -
GRAILQA 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 99.98 0.02 0.00
WebQsp 3.48 1.95 94.57 100.00 0.00 0.00 3.36 2.07 94.57 100.00 0.00 0.00
Russ 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
MTOP 0.00 100.00 0.00 100.00 0.00 0.00 0.00 100.00 0.00 100.00 0.00 0.00
WikiTableQuestions 48.00 31.15 20.86 100.00 0.00 0.00 47.08 31.70 21.22 99.98 0.02 0.00
WikiSQL 61.49 26.00 12.51 100.00 0.00 0.00 59.57 27.43 13.00 99.96 0.03 0.01
ComWebQ 0.85 16.02 83.13 100.00 0.00 0.00 0.85 13.07 86.08 99.43 0.57 0.00
HybridQA - - - - - - - - - - - -
FeTaQA 65.40 28.01 6.59 100.00 0.00 0.00 63.26 29.51 7.24 100.00 0.00 0.00
DART 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
ToTTo - - - - - - - - - - - -
MultiWoZ 100.00 0.00 0.00 98.71 1.29 0.00 24.82 74.93 0.24 0.00 100.00 0.00
KVRET 66.14 33.86 0.00 100.00 0.00 0.00 66.14 33.86 0.00 100.00 0.00 0.00
SParC - - - - - - - - - - - -
CoSQL - - - - - - - - - - - -
SQA 62.54 30.92 6.54 100.00 0.00 0.00 61.37 32.05 6.58 93.69 5.68 0.63
TabFact 64.59 28.01 7.40 100.00 0.00 0.00 62.55 29.35 8.10 100.00 0.00 0.00
FEVEROUS - - - - - - - - - - - -
SQL2Text 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
Logic2Text 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00 100.00 0.00 0.00
Table 16: Input and output length for each task’s test set.
0 1000 2000 3000 4000
input tokens0.00000.00020.00040.00060.00080.00100.00120.00140.0016DensityLength distribution & model performance(T5 large)
3638404244464850
Acc.wikitq
Figure 6: Length effect on WikiTableQuestion.
style), make our input in that format and directly
test on T0 3B. The speciﬁc instructions are shown
below.
Spider
Given database schema "[linearized database
schema]". Can you tell me the SQL for "[request
]"?
WikiTQ
I know that the answer to "[request]" is in "[
linearized table]". Can you tell me what it is?
DART
Put the triples together to form a sentence: [
relation triples]
MultiWoZ
Known ontology "[ontology]", the dialogue state
when "[dialogue history and current request]" is
given
TabFact
Suppose "[linearized table]" Can we infer that
"[statement]"?
SQL2TextParaphrase "[SQL]" to natural language:
D.4 GPT3 and Codex Details
D.4.1 Hyperparameter Settings
Temperature For GPT3 and Codex, we set the
decoding temperature to 0 (i.e., greedy decoding
without sampling) for Spider, WikiTQ, MultiWoZ
and TabFact. We observe a drop of 10% in the
exact match metric when set the temperature to 1
by default in OpenAI. For Codex, we tune the tem-
perature from 0 to 1 in a step of 0.1 for DART,
SQL2Text, and no signiﬁcant difference is ob-
served. For GPT3, we do not tune on that to stay
within our budget.
Max output length We set max output length to
256 for Spider, WikiTQ, MultiWoZ and SQL2Text,
while 4 for TabFact to contain more length in the
input side(the concept of max length in GPT3 and
Codex is the sum of input tokens length and output
tokens length). We set “\n” as the stop token.
D.4.2 Prompts
We use simple prompt words for each task to con-
catenate the request, linearized structured knowl-
edge, and context together. For example, for
each example in WikiTQ, we format it as “ ex-
amples \n\n[linearized table] || Write a answer for
[request] \nThe answer is:”, and make GPT3 and
Codex make the completion as prediction. We
do experiments on Spider with different format of
forming structured knowledge (e.g., linearization,
description), but get a similar result. Better us-age of GPT3 and Codex under the UNIFIED SKG
framework is an interesting direction.
D.5 Human Evaluation
Participants of our human evaluation are eight of
the authors of this paper. They are familiar with
the tasks being evaluated. The human evaluation
guideline is shown below.
## General Guideline
1. Each line is a dev set sample, with some
inputs (detailed below), a human reference (
seq_out) shown in blue, and three model outputs
named model1, model2, and model3.
2. Each model output receives a 0-1 score (0
stands for incorrect, and 1 stands for correct).
By "correct" we mean "responding to the user
request properly and correctly, without grammar
or wording mistakes".
3. When an output is incorrect, you specify the
type(s) of error, e.g., 1) missing information,
2) contradiction, 3) hallucination, and 4)
ungrammatical.
## Task-Specific Details
### DART
1. Task: triples-to-text generation.
2. struct_in: a set of relation-triples joined
by/grave.ts1/grave.ts1|/grave.ts1/grave.ts1. Each relation-triple is of form /grave.ts1/grave.ts1
entityA : relation : entityB /grave.ts1/grave.ts1.
### FeTaQA
1. Task: free-form QA
2. question: a question about the table.
3. table: a table represented as a dictionary:
{"header": [header item, ...], "rows": [[cell
value, ...], ...]}.
4. meta: table_page_title | table_section_title
### KVRET
1. Task: dialogue system
2. dialogue: a dialogue represented as a
dictionary: {"driver": [request1, ...], "
assistant": [response1, ...]}, the last response
of the assistant is the human reference.
3. kb: a knowledge base represented as a
dictionary: {"header": [header item, ...], "rows
": [[cell value, ...], ...]}.
### Logic2Text
1. Task: logic expression to text translation
2. table: a table represented as a dictionary:
{"caption": table caption, "header": [header
item, ...], "rows": [[cell value, ...], ...]}.
3. logic_str: logic expression of a statement.
### SQL2Text
1. Task: SQL to text translation
2. query: SQL.
### ToTTo
1. Task: highlighted-table-to-text generation.
2. table_page_title and section: table meta
information.
3. Visualization of highlighted tables is
provided in /grave.ts1/grave.ts1totto_vis/ /grave.ts1/grave.ts1.D.6 Hyperparameters
Shown in Table 17. For semantic parsing tasks, the
decoding was done under the greedy search, where
we set the beam size to 1 specially. For tasks with
a long linearized sequence, we used 1024 as input
length to hold the maximum of input; reasons are
explained in App. C.
E Training Details
Here we show comparisons of ﬁnetuning and
preﬁx-tuning on aspect of training. For preﬁx-
tuning, we use random initialization as done by Li
and Liang (2021). In general, preﬁx-tuning needs
more steps than ﬁnetuning but has the ability to
reach comparable results with continued training.
F Task Uniﬁcation
F.1 Term Deﬁnition
Highlighted tables A highlighted table contains
a table, table metadata (such as the title), and a set
of highlighted cells which entails the text descrip-
tion (Parikh et al., 2020).
Relation-triples Relation triples are a set of
subject-predicate-object triples to capture rich re-
lationships in the data. Many data-to-text tasks
such as DART (Nan et al., 2021b) take these rela-
tion triples as inputs and generate natural language
from them.
Knowledge Graph A knowledge graph is a
multi-relational graph composed of entities (nodes)
and relations (different types of edges). Each edge
is represented as a triple of the form (head entity,
relation, tail entity), also called a fact, indicating
that two entities are connected by a speciﬁc relation
(Wang et al., 2017).
Dialogue State and Ontology A dialogue state
stat any turn tin a dialogue comprises the sum-
mary of the dialogue history until turn t, such that
stcontains all sufﬁcient information for the system
to choose the next action. (Williams et al., 2016)
Speciﬁcally, it captures the user goals in the con-
versation in the form of (slot, value) pairs. The
set of possible slots is predeﬁned in the ontology
O, typically domain-dependent, while the values
assumed by each slots are provided by the user as
a dialogue goal.Task type Task Input length Batch size Beam size
Semantic ParsingSpider (Yu et al., 2018) 512 32 1
GrailQA (Gu et al., 2021) 512 32 4
WebQSP (Yih et al., 2016) 1024 32 4
MTOP (Li et al., 2021) 1024 32 4
Question AnsweringWikiSQL (Zhong et al., 2017) 1024 128 4
WikiTQ (Pasupat and Liang, 2015) 1024 128 4
CompWebQ (Talmor and Berant, 2018) 1024 32 4
HybridQA (Chen et al., 2020c) 1024 32 4
MultiModalQA (Talmor et al., 2021) 1024 32 4
FeTaQA (Nan et al., 2021a) 512 32 4
Data-to-TextDART (Nan et al., 2021b) 512 32 4
ToTTo (Parikh et al., 2020) 512 32 4
MultiWoZ2.1 (Eric et al., 2019) 1024 32 4
KVRET (Eric et al., 2017) 1024 32 4
Conversational SParC (Yu et al., 2019b) 512 32 1
CoSQL (Yu et al., 2019a) 512 32 1
SQA (Iyyer et al., 2017) 1024 128 4
Fact VeriﬁcationTabFact (Chen et al., 2020b) 1024 128 4
FEVEROUS (Aly et al., 2021) 1024 32 4
High-ﬁdelity NLGSQL2Text (Shu et al., 2021) 512 32 4
Logic2Text (Chen et al., 2020d) 512 32 4
Table 17: Hyperparameters for each SKG task.
F.2 Linearization
•Tables. Following Liu et al. (2021), we linearize
the table into a sequence. By inserting several
special tokens to indicate the table boundaries, a
linearized table can be represented as “col: c1, ...,
cNrow 1 : r1row 2 : r2...rM”,NandMare
the number of columns and rows.
•Highlighted tables. Following Parikh et al.
(2020), we represent each highlighted cell by
concatenating its value, column headers, and row
headers. The table is represented as the concate-
nation of the page title, section title, and repre-
sentations of all highlighted cells.
•Relation-triples and knowledge graphs. Fol-
lowing Nan et al. (2021b), each relation-triple
is linearized as “ sub:rela:obj”, and different
triples are joined by “ | ”. The subgraph retrieved
from the knowledge graph is treated as a list of
relation-triples and we use the same formulation.
•Ontology. Following Hosseini-Asl et al. (2020)
and Lin et al. (2021), for each slot in ontology,
each slot along with its all possible values is for-
matted as “ slot:value 1, ...value slotn”, different
slot-values are joined by “ | ”
F.3 Output Format
When the output is natural language orformal lan-
guage we do not modify it because it is already insequence format; a set of answers , we use a comma
followed by a space to join the answers; a Boolean
value , we map True to “entailed” and False to “re-
futed”; a dialogue state , we follow Hosseini-Asl
et al. (2020) to place its slot-value pairs sequen-
tially.
G Input and Output Examples for Each
Task
G.1 Spider
Structured Input:
| concert_singer | stadium : stadium_id ,
location , name , capacity , highest , lowest ,
average | singer : singer_id , name , country ,
song_name , song_release_year , age , is_male |
concert : concert_id , concert_name , theme ,
stadium_id , year | singer_in_concert :
concert_id , singer_id
Request Input:
How many singers do we have?
Sequence Output:
select count(*) from singer
G.2 GRAILQA
Structured Input:
soviet red army: m.06dr9 | organization.
organization.founders government.
governmental_body.jurisdiction organization.
organization_founder.organizations_foundedTask Finetune Preﬁx-tuning
Spider 16500 100000
GrailQA 17000 78000
WebQSP 1500 8000
MTOP 30000 60000
WikiSQL 8500 80000
WikiTQ 1500 16000
CompWebQ 3500 27000
HybridQA 7000 30000
MultiModalQA 6000 40000
FeTaQA 11000 20000
DART 7000 250000
ToTTo 12000 >250000
MultiWoZ2.1 6000 40000
KVRET 4000 40000
SParC 2000 6400
CoSQL 38000 100000
SQA 27000 >250000
TabFact 8000 210000
FEVEROUS 1200 40000
SQL2Text 3000 10000
Logic2Text 3500 10000
Table 18: The comparison of approximate training
steps ﬁnetuning and preﬁx-tuning used to reach the de-
cent performance on T5 base. >250000 means we stop
the training due to time limitation. Preﬁx-tuning needs
more steps to converge and converges to comparable
performances.
military.military_service.military_person
government.political_party_tenure government.
national_anthem_of_a_country visual_art.
art_subject.artwork_on_the_subject government.
government_agency government.
governmental_jurisdiction.government people.
deceased_person.place_of_burial people.
deceased_person.date_of_death people.person.
children people.person.parents people.person.
height_meters government.
government_position_held.office_holder
government.government people.person people.
person.sibling_s people.person.quotations people.
person.gender
Request Input:
the person who founded the soviet red army also
founded what government agency?
Sequence Output:
(AND government.government_agency (JOIN
organization.organization.founders (JOIN (R
organization.organization.founders) m.06dr9)))
G.3 CompWebQ
Structured Input:
Liam Hemsworth celebrities.celebrity.
sexual_relationships..celebrities.
romantic_relationship.celebrity Liam Hemsworth |
Liam Hemsworth type.object.type tv.tv_actor |
Liam Hemsworth film.actor.film..film.performance.
character Billy The Kid | Liam Hemsworth film.
actor.film..film.performance.film The Hunger
Games |(omitted to save space)
| Liam Hemsworth type.object.type celebrities.
celebrity | Cut Bank film.film.cinematography
Ben Richardson | Cut Bank film.film.language
English Language | Cut Bank film.film.edited_by
Carol Littleton | Liam Hemsworth film.actor.film
..film.performance.film The Hunger Games:
Mockingjay, Part 1 | Liam Hemsworth film.actor.
film..film.performance.film Timeless
Request Input:
What movie was produced by Dan Cohen and
features Liam Hemsworth as an actor?
Sequence Output:
Cut Bank
G.4 WebQsp
Structured Input:
Spain: m.06mkj | m.06mkj location.location.
contains m.0g3qgy | m.06mkj location.location.
contains m.02qf5mh | m.0j5_3sv government.
government_position_held.
office_position_or_title m.0j5_3sz | m.06mkj
location.location.contains m.02zb43k |
(omitted to save space)
| m.06mkj government.governmental_jurisdiction.
governing_officials m.010wswjc | m.06mkj
location.location.contains m.09k5hy | m.010wswjc
government.government_position_held.
office_position_or_title m.0j5_3sz | m.06mkj
location.location.contains m.02z98t5 | m.06mkj
location.location.contains m.03qcr60
Request Input:
what is the king of spain /quotesingle.Vars name?
Sequence Output:
(JOIN (R government.government_position_held.
office_holder) (AND (JOIN government.
government_position_held.time_macro 2015^^http
://www.w3.org/2001/XMLSchema#date) (AND (JOIN
government.government_position_held.
office_position_or_title m.0j5_3sz) (JOIN (R
government.governmental_jurisdiction.
governing_officials) m.06mkj))))
G.5 MTOP
Structured Input:
IN:GET: MESSAGE, WEATHER, ALARM, INFO_RECIPES,
STORIES_NEWS, REMINDER, RECIPES, EVENT,
CALL_TIME, LIFE_EVENT, INFO_CONTACT, CONTACT,
TIMER, REMINDER_DATE_TIME, AGE, SUNRISE,
EMPLOYER, EDUCATION_TIME, JOB, AVAILABILITY,
(omitted to save space)
IN:PREVIOUS: TRACK_MUSIC | IN:HOLD: CALL | IN:
SKIP: TRACK_MUSIC | IN:LIKE: MUSIC | IN:RESTART:
TIMER | IN:RESUME: TIMER, CALL, MUSIC | IN:
MERGE: CALL | IN:REPLAY: MUSIC | IN:LOOP: MUSIC
| IN:STOP: MUSIC, SHUFFLE_MUSIC | IN:UNLOOP:
MUSIC | IN:CANCEL: MESSAGE, CALL | IN:REWIND:
MUSIC | IN:REPEAT: ALL_MUSIC, ALL_OFF_MUSIC |
IN:FAST: FORWARD_MUSIC | IN:DISLIKE: MUSIC | IN:
DISPREFER: | IN:HELP: REMINDER | IN:FOLLOW:
MUSICRequest Input:
call Nicholas and Natasha
Sequence Output:
[IN:CREATE_CALL [SL:CONTACT Nicholas ] [SL:
CONTACT Natasha ] ]
G.6 WikiSQL
Structured Input:
col : player | no. | nationality | position |
years in toronto | school/club team row 1 :
antonio lang | 21 | united states | guard-
forward | 1999-2000 | duke row 2 : voshon lenard
| 2 | united states | guard | 2002-03 |
minnesota row 3 : martin lewis | 32, 44 | united
states | guard-forward | 1996-97 | butler cc (
ks) row 4 : brad lohaus | 33 | united states |
forward-center | 1996 | iowa row 5 : art long |
42 | united states | forward-center | 2002-03 |
cincinnati row 6 : john long | 25 | united
states | guard | 1996-97 | detroit row 7 : kyle
lowry | 3 | united states | guard | 2012-present
| villanova
Request Input:
what position does the player who played for
butler cc (ks) play?
Sequence Output:
guard-forward
G.7 WikiTableQuestions
Structured Input:
col : team | county | wins | years won row 1 :
greystones | wicklow | 1 | 2011 row 2 :
ballymore eustace | kildare | 1 | 2010 row 3 :
maynooth | kildare | 1 | 2009 row 4 : ballyroan
abbey | laois | 1 | 2008 row 5 : fingal ravens |
dublin | 1 | 2007 row 6 : confey | kildare | 1
| 2006 row 7 : crettyard | laois | 1 | 2005 row
8 : wolfe tones | meath | 1 | 2004 row 9 :
dundalk gaels | louth | 1 | 2003
Request Input:
which team won previous to crettyard?
Sequence Output:
wolfe tones
G.8 HybridQA
Structured Input:
col : position | athlete | nationality | time
row 1 : 1 | patrick makau musyoki | kenya |
2:03.38 row 2 : 2 | stephen kwelio chemlany |
kenya | 2:07.55 row 3 : 3 | edwin kimaiyo |
kenya | 2:09.50 row 4 : 4 | felix limo | kenya |
2:10.38 row 5 : 5 | scott overall | united
kingdom | 2:10.55 row 6 : 6 | ricardo serrano |
spain | 2:13.32 row 7 : 7 | pedro nimo | spain |
2:13.34 row 8 : 8 | simon munyutu | france |2:14.20 row 9 : 9 | driss el himer | france |
2:14.46 row 10 : 10 | hendrick ramaala | south
africa | 2:16.00passages: ricardo serrano (
athlete): at the 2011 iaaf world cross country
championships he was 89th overall . his marathon
debut followed later that year and he was sixth
at the 2011 berlin marathon with a time of
2:13.32 hours . | spain: with an area of 505,990
km2 ( 195,360 sq mi ) , spain is the largest
country in southern europe , the second largest
country in western europe and the european union
, and the fourth largest country in the
european continent . by population ( about 47
million ) , spain is the sixth largest in europe
and the fifth in the european union . |
Request Input:
what place was achieved by the person who
finished the berlin marathon in 2:13.32 in 2011
the first time he competed in a marathon ?
Sequence Output:
sixth
G.9 MultiModalQA
Structured Input:
ben piazza | filmography col : year | title |
role | notes row 1 : 1957 | a dangerous age |
david | row 2 : 1959 | the hanging tree | rune |
row 3 : 1962 | no exit | camarero | row 4 :
1970 | tell me that you love me, junie moon |
jesse | row 5 : 1972 | the outside man | desk
clerk | row 6 : 1973 | the candy snatchers |
avery | row 7 : 1976 | the bad news bears | bob
whitewood | row 8 : 1977 | i never promised you
a rose garden | jay blake | row 9 : 1979 |
nightwing | roger piggott | row 10 : 1979 | the
concorde ... airport /quotesingle.Var79 | associate | tv
version, uncredited row 11 : 1980 | the blues
brothers | father | row 12 : 1982 | waltz across
texas | bill wrather | row 13 : 1985 | mask |
mr. simms | row 14 : 1988 | clean and sober |
kramer | row 15 : 1990 | rocky v | doctor |
uncredited row 16 : 1991 | guilty by suspicion |
darryl zanuck |
Request Input:
for which film did ben piazza play the role of
mr. simms?
Sequence Output:
mask
G.10 FeTaQA
Structured Input:
andy karl | awards and nominations col : year |
award | category | work | result row 1 : 2013 |
drama desk award | outstanding featured actor in
a musical | the mystery of edwin drood |
nominated row 2 : 2013 | broadway.com audience
awards | favorite onstage pair (with jessie
mueller) | the mystery of edwin drood |
nominated row 3 : 2014 |(omitted to save space)
| groundhog day | nominated row 15 : 2017 | tony
award | best actor in a musical | groundhog day
| nominated row 16 : 2017 | drama desk award |
outstanding actor in a musical | groundhog day |
won row 17 : 2017 | drama league award |
distinguished performance | groundhog day |
nominated row 18 : 2017 | outer critics circle
award | outstanding actor in a musical |
groundhog day | won
Request Input:
when did andy karl win the olivier award and for
which of his work?
Sequence Output:
for his performance in groundhog day, andy karl
received the 2017 olivier award for best actor
in a musical.
G.11 DART
Structured Input:
Mars Hill College : joined : 1973 | Mars Hill
College : location : Mars Hill, North Carolina
Request Input:
None
Sequence Output:
A school from Mars Hill, North Carolina, joined
in 1973.
G.12 ToTTo
Structured Input:
<page_title> List of Governors of South Carolina
</page_title> <section_title> Governors under
the Constitution of 1868 </section_title> <table
> <cell> 76 <col_header> # </col_header> <
col_header> 74 </col_header> <col_header> 75 </
col_header> </cell> <cell> Daniel Henry
Chamberlain <col_header> Governor </col_header>
<row_header> 76 </row_header> </cell> <cell>
December 1, 1874 <col_header> Took Office </
col_header> <row_header> 76 </row_header> </cell
> </table>
Request Input:
None
Sequence Output:
Daniel Henry Chamberlain was the 76th Governor
of South Carolina from 1874.
G.13 MultiWoZ2.1
Structured Input:hotel-pricerange: cheap, dontcare, expensive,
moderate; hotel-type: guesthouse, hotel; hotel-
parking: dontcare, free, no, yes; hotel-book day:
friday, monday, saturday, sunday, thursday,
tuesday, wednesday; hotel-book people: 1, 2, 3,
4, 5, 6, 7, 8; hotel-book stay: 1, 2, 3, 4, 5, 6,
7, 8; hotel-area: centre, dontcare, east, north,
south, west; hotel-stars: 0, 1, 2, 3, 4, 5,
dontcare; hotel-internet: dontcare, no, yes;
hotel-name: none; train-destination: none; train-
day: dontcare, friday, monday, saturday, sunday,
thursday, tuesday, wednesday; train-departure:
none; train-arriveby: none; train-book people: 0,
1, 10, 15, 2, 3, 4, 5, 6, 7, 8, 9; taxi-
destination: none; taxi-departure: none; taxi-
leaveat: none; train-leaveat: none; attraction-
area: cambridge, centre, dontcare, east, north,
south, west; restaurant-pricerange: cheap,
dontcare, expensive, moderate; restaurant-area:
centre, east, north, south, west; restaurant-
food: none; attraction-name: none; restaurant-
name: none; attraction-type: architecture, boat,
church, cinema, college, concerthall,
entertainment, hotspot, multiple sports, museum,
nightclub, park, special, swimmingpool, theatre
; restaurant-book day: friday, monday, saturday,
sunday, thursday, tuesday, wednesday;
restaurant-book people: 1, 2, 3, 4, 5, 6, 7, 8;
restaurant-book time: none; taxi-arriveby: none;
Request Input:
i am looking to book a train that is leaving
from cambridge to bishops stortford on friday .
Context:
booking was successful . reference number is :
bmukptg6 . can i help you with anything else
today ? | friday and can you book it for me and
get a reference number ? | what day will you be
staying ? | could you book the wartworth for 1
night , 1 person ? | if you would like something
cheap , i recommend the allenbell . for
something moderate -ly priced , i would
recommend the warkworth house . | that does not
matter as long as it has free wifi and parking .
| i can help you with that . what is your price
range ? | i need to book a hotel in the east
that has 4 stars . | none
Sequence Output:
hotel pricerange none, hotel type none, hotel
parking yes, hotel book day friday, hotel book
people 1, hotel book stay 1, hotel area east,
hotel stars 4, hotel internet yes, hotel name
wartworth, train destination bishops stortford,
train day friday, train departure cambridge,
train arriveby none, train book people none,
taxi destination none, taxi departure none, taxi
leaveat none, train leaveat none, attraction
area none, restaurant pricerange none,
restaurant area none, restaurant food none,
attraction name none, restaurant name none,
attraction type none, restaurant book day none,
restaurant book people none, restaurant book
time none, taxi arriveby none
G.14 KVRET
Structured Input:col : poi | poi_type | address | distance |
traffic_info row 1 : sigona farmers market |
grocery store | 638 amherst st | 3 miles | car
collision nearby row 2 : cafe venetia | coffee
or tea place | 269 alger dr | 1 miles | car
collision nearby row 3 : 5672 barringer street |
certain address | 5672 barringer street | 5
miles | no traffic row 4 : valero | gas station
| 200 alester ave | 2 miles | road block nearby
row 5 : stanford childrens health | hospital |
899 ames ct | 5 miles | moderate traffic row 6 :
palo alto garage r | parking garage | 481
amaranta ave | 1 miles | moderate traffic row 7 :
teavana | coffee or tea place | 145 amherst st
| 1 miles | road block nearby row 8 : willows
market | grocery store | 409 bollard st | 5
miles | no traffic
Request Input:
ok, please give me directions via a route that
avoids all heavy_traffic.
Context:
valero is located at 200_alester_ave. | address
to the gas_station
Sequence Output:
since there is a road_block_nearby, i found
another route for you and i sent it on your
screen
G.15 SParC
Structured Input:
| flight_2 | airlines : uid , airline ,
abbreviation , country | airports : city ,
airportcode , airportname , country ,
countryabbrev | flights : airline , flightno ,
sourceairport , destairport
Request Input:
What is the country corresponding it?
Context:
Of these, which is Jetblue Airways? | What are
all the airlines?
Sequence Output:
select country from airlines where airline = "
JetBlue Airways"
G.16 CoSQL
Structured Input:
| car_1 | continents : contid , continent |
countries : countryid , countryname , continent
| car_makers : id , maker , fullname , country |
model_list : modelid , maker , model |
car_names : makeid , model , make | cars_data :
id , mpg , cylinders , edispl , horsepower ,
weight , accelerate , year
Request Input:
Which models are the most popular?Context:
How many car models are produced by each maker?
List the count and the maker full name.
Sequence Output:
select t1.model from model_list as t1 join
car_makers as t2 on t1.maker = t2.id group by t2.
id order by count ( * ) desc limit 1
G.17 SQA
Structured Input:
col : date | number | type | captain |
casualties | sunk by row 1 : 4 may 1943 | u-209
| viic | heinrich brodda | 46 | catalina flying-
boat of no. 5 squadron row 2 : 5 may 1943 | u
-638 | viic | oskar staudinger | 44 | hms
sunflower row 3 : 5 may 1943 | u-531 | ixc/40 |
herbert neckel | 54 | hms vidette row 4 : 6 may
1943 | u-192 | ixc/40 | werner happe | 55 | hms
loosestrife row 5 : 6 may 1943 | u-125 | ixc |
ulrich folkers | 54 | "hms oribi | hms snowflake
" row 6 : 6 may 1943 | u-630 | viic | werner
winkler | 47 | hms vidette row 7 : 6 may 1943 |
u-438 | viic | heinrich hensohn | 48 | hms
pelican
Request Input:
which captain was not oskar staudinger?
Context:
who were the captains of those boats? | what
boats were lost on may 5?
Sequence Output:
herbert neckel
G.18 TabFact
Structured Input:
col : round | clubs remaining | clubs involved |
winners from previous round | new entries this
round | leagues entering at this round row 1 :
first round | 156 | 86 | none | 86 | tff third
league & turkish regional amateur league row 2 :
second round | 113 | 108 | 43 | 65 | s\u00fcper
lig & tff first league & tff second league row
3 : third round | 59 | 54 | 54 | none | none row
4 : fourth round | 32 | 32 | 27 | 5 | s\
u00fcper lig row 5 : fifth round | 16 | 16 | 16
| none | none row 6 : group stage | 8 | 8 | 8 |
none | none row 7 : semi - finals | 4 | 4 | 4 |
none | none row 8 : final | 2 | 2 | 2 | none |
none
Request Input:
during the third round of the turkish cup ,
there be no new entry during that stage
Sequence Output:
entailedG.19 FEVEROUS
Structured Input:
col : no. | title | narrator | aired between |
original air date | us viewers row 1 : 1 | "
magic is coming" | giancarlo esposito | "a land
without magic" "broken" | september 30, 2012
(2012-09-30) | 6.04 row 2 : 2 | "the price of
magic" | alan dale | "selfless, brave and true"
"lacey" | april 14, 2013 (2013-04-14) | 5.17 row
3 : 3 | "journey to neverland" | alfred molina
| "and straight on /quotesingle.Vartil morning" "the heart of
the |
(omitted to save space)
| "dark swan rises: a once upon a time fan |
howard parker | "operation mongoose, part 2" "
the | september 27, 2015 (2015-09-27) | 3.20 row
8 : 8 | "evil reigns once more" | howard parker
| "an untold story" "the savior" | september 25,
2016 (2016-09-25) | 2.86 row 9 : 9 | "the final
battle begins" | howard parker | "the song in
your heart" "the final battle" | may 14, 2017
(2017-05-14) | 2.82
Request Input:
list of once upon a time (debuted october 23,
2011) episodes consists of 10 specials, one of
the narrators alan dale.
Sequence Output:
refuted
G.20 SQL2Text
Structured Input:
None
Request Input:
SELECT Fname FROM FACULTY WHERE Rank = "
Professor" ORDER BY Fname
Sequence Output:
What are the first names for all faculty
professors, ordered by first name?
G.21 Logic2Text
Structured Input:
Caption: list of tallest structures in the world
- 300 to 400 metres | planned pinnacle height,
expected year of completion, structural type,
country, town
Request Input:
eq { count { filter_eq { all_rows ; expected
year of completion ; 2006 } } ; 3 } = true
Sequence Output:
three of the tallest structures will be
completed in 2006 .H Case study
H.1 Spider case
H.1.1 Case 1:
Structured Input:
| concert_singer | stadium : stadium_id ,
location , name , capacity , highest , lowest ,
average | singer : singer_id , name , country ,
song_name , song_release_year , age , is_male |
concert : concert_id , concert_name , theme ,
stadium_id , year | singer_in_concert :
concert_id , singer_id
Request Input:
What are the names of all stadiums that did not
have a concert in 2014?
Sequence Output:
select name from stadium except select t2.name
from concert as t1 join stadium as t2 on t1.
stadium_id = t2.stadium_id where t1.year = 2014
T5-base prediction (incorrect):
select name from stadium except select
stadium_name from concert where year = 2014
T5-large prediction (correct):
select name from stadium except select t2.name
from concert as t1 join stadium as t2 on t1.
stadium_id = t2.stadium_id where t1.year = 2014
T5-3B prediction (correct):
select name from stadium except select t2.name
from concert as t1 join stadium as t2 on t1.
stadium_id = t2.stadium_id where t1.year = 2014
H.1.2 Case 2:
Structured Input:
| concert_singer | stadium : stadium_id ,
location , name , capacity , highest , lowest ,
average | singer : singer_id , name , country ,
song_name , song_release_year , age , is_male |
concert : concert_id , concert_name , theme ,
stadium_id , year | singer_in_concert :
concert_id , singer_id
Request Input:
What is the name and capacity for the stadium
with highest average attendance?
Sequence Output:
select name, capacity from stadium order by
average desc limit 1
T5-base prediction (incorrect):
select name, capacity from stadium order by avg(
amount) desc limit 1
T5-large prediction (correct):
select name, capacity from stadium order by
average desc limit 1T5-3B prediction (correct):
select name, capacity from stadium order by
average desc limit 1
H.1.3 Case 3:
Structured Input:
| pets_1 | student : stuid , lname , fname , age
, sex , major , advisor , city_code | has_pet :
stuid , petid | pets : petid , pettype ( cat ,
dog ) , pet_age , weight
Request Input:
Find the first name of students who have cat or
dog pet.
Sequence Output:
select distinct t1.fname from student as t1 join
has_pet as t2 on t1.stuid = t2.stuid join pets
as t3 on t3.petid = t2.petid where t3.pettype =
/quotesingle.Varcat/quotesingle.Varor t3.pettype = /quotesingle.Vardog/quotesingle.Var
T5-base prediction (incorrect):
select t1.fname from student as t1 join has_pet
as t2 on t1.stuid = t2.stuid where t2.pettype =
"cat" or t2.pettype = "dog"
T5-large prediction (incorrect):
select t1.fname from student as t1 join has_pet
as t2 on t1.stuid = t2.stuid where t2.pettype =
"cat" or t2.pettype = "dog"
T5-3B prediction (correct):
select t1.fname from student as t1 join has_pet
as t2 on t1.stuid = t2.stuid join pets as t3 on
t2.petid = t3.petid where t3.pettype = "cat" or
t3.pettype = "dog"
H.2 FeTaQA case
Structured Input:
te aroha (new zealand electorate) | 1890
election col : party | party | candidate | votes
| % | [non utf-8 token] row 1 : - | independent
| william shepherd allen | 786 | 56.34 | - row
2 : - | liberal | william fraser | 609 | 43.65 |
- row 3 : majority | majority | majority | 175
| 12.54 | - row 4 : turnout | turnout | turnout
| 1,395 | 48.60 | - row 5 : registered electors
| registered electors | registered electors |
2,870 | - | -
Request Input:
what were the voting results in the te aroha
election by the candidate?
Sequence Output:
votes for allen and fraser were 786 and 609
respectively.
T5-base prediction (incorrect):
the te aroha election was won by william
shepherd allen with a majority of 175.T5-large prediction (incorrect):
in the te aroha election, allen won with a
majority of 175 votes.
T5-3B prediction (correct):
in the 1890 te aroha election, the result was a
victory for the independent candidate, william
shepherd allen, who received 786 votes against
609 for the liberal candidate, william fraser.
H.3 KVRET case
H.3.1 Case 1:
Structured Input:
col : location | monday | tuesday | wednesday |
thursday | friday | saturday | sunday | today
row 1 : danville | clear skies, low of 30f, high
of 50f | cloudy, low of 20f, high of 40f |
raining, low of 80f, high of 100f | dew, low of
80f, high of 90f | clear skies, low of 70f, high
of 80f | frost, low of 40f, high of 60f |
overcast, low of 20f, high of 30f | monday row 2
:... row 6 : alhambra | windy, low of 90f, high
of 100f | hail, low of 90f, high of 100f |
clear skies, low of 80f, high of 90f | rain, low
of 70f, high of 80f | stormy, low of 70f, high
of 80f | rain, low of 60f, high of 70f | hail,
low of 40f, high of 60f | monday ...
Request Input:
alhambra please.
Context Input:
what city are you inquiring about? | is it going
to be raining this weekend
Sequence Output:
it will be raining on saturday and hailing on
sunday in alhambra.
T5-base prediction (correct):
it will be raining on saturday and hailing on
sunday in alhambra.
T5-large prediction (incorrect):
it will not rain in alhambra this weekend.
T5-3B prediction (incorrect):
there will be no rain this weekend in alhambra.
H.3.2 Case 2:
Structured Input:
col : event | time | date | room | agenda |
party row 1 : tennis activity | 5pm | wednesday
| - | - | mother row 2 : taking medicine | 9am |
the 3rd | - | - | sister row 3 : swimming
activity | 11am | friday | - | - | father row 4 :
conference | 6pm | the 8th | 100 conference
room | go over budget | infrastructure team row
5 : meeting | 4pm | the 10th | conference room
102 | go over quarterly report | infrastructure
team row 6 : dentist appointment | 10am | the 13
th | - | - | jeff row 7 : dentist appointment |
11am | the 5th | - | - | jeff row 8 : lab
appointment | 2pm | the 4th | - | - | sisterRequest Input:
when is the date and time of my
dentist_appointment?
Context Input:
None
Sequence Output:
your dentist_appointment is at 10am on the_13th.
T5-base prediction (correct):
you have two dentist_appointments scheduled. one
on the_5th at 11am and another on the_13th at
10am.
T5-large prediction (correct):
your dentist_appointment is on the_13th at 10 am.
T5-3B prediction (incorrect):
you have three dentist_appointments scheduled.
one on the_5th at 11am, another on the_13th at
10am, and the last is on the_5th at 11am.
H.4 SQL2Text case
Query:
SELECT company , main_industry FROM company
WHERE headquarters != /quotesingle.VarUSA/quotesingle.Var
Sequence Out:
Show the company name and the main industry for
all companies whose headquarters are not from
USA.
T5-base prediction (incorrect):
What are the companies and main industries of
companies whose headquarters are not "USA"?
T5-large prediction (correct):
What are the companies and main industries of
the companies whose headquarters are not located
in the United States?
T5-3B prediction (incorrect):
What are the main industries of companies whose
headquarters are not in the USA?
H.5 DART case
Structured Input:
Cotto : eattype : coffee shop | Cotto : food :
English | Cotto : pricerange : more than [non
utf-8 token]30 | Cotto : customer rating : high
| Cotto : area : city centre | Cotto : near :
The Portland Arms
Sequence Output:
The Cotto is a highly rated English coffee shop
in the city centre near The Portland Arms, and
prices range more than [non utf-8 token]30.T5-base prediction (correct):
Cotto is a coffee shop providing English food in
the more than [non utf-8 token]30 price range.
It is located in the city centre. It is near The
Portland Arms. Its customer rating is high.
T5-large prediction (correct):
Cotto is a coffee shop providing English food in
the more than [non utf-8 token]30 price range.
It is located in the city centre. It is near The
Portland Arms. Its customer rating is high.
T5-3B prediction (correct):
Cotto is a coffee shop located in the city
centre near The Portland Arms. It serves English
food with a price range of more than [non utf-8
token]30 and has a high customer rating.
H.6 Logic2Text case
Structured Input:
Caption: 2008 san diego chargers season | week,
date, time, opponent, result, game site, nfl
recap, record
Request Input:
eq { count { filter_eq { filter_eq { all_rows ;
game site ; qualcomm stadium } ; time ; 5:15 pm }
} ; 3 } = true
Sequence Output:
in the 2008 san diego chargers season , among
the games that were played in qualcomm stadium ,
3 of them started at 5:15 pm .
T5-base prediction (incorrect):
in the 2008 san diego chargers season, when the
game was at qualcomm stadium, there were three
times the time was 5:15 pm.
T5-large prediction (incorrect):
in the 2008 san diego chargers season, when the
game was at qualcomm stadium, there were 3 times
the time was 5:15 pm.
T5-3B prediction (correct):
in the 2008 san diego chargers season, among the
games played at qualcomm stadium, 3 of them
started at 5:15 pm.
H.7 ToTTo case
Structured Input: See Figure 7.
Sequence Output:
Alisson Perticheto placed 18th at the 2013
Junior Worlds, 17th at the 2014 Four Continents
and 16th at the 2015 Four Continents.
T5-base prediction (incorrect):
Alisson Perticheto finished 18th at the Junior
Worlds and 17th at the Four Continents.Figure 7: Visualized highted table for ToTTo case 1.
T5-large prediction (incorrect):
Alisson Perticheto placed 17th at the 2014 Four
Continents and 16th at the 2015 Junior Worlds.
T5-3B prediction (correct):
Alisson Perticheto finished 17th at the 2014
Four Continents, 16th at the 2015 Four
Continents, and 18th at the 2013 Junior Worlds.
I Natural Language Template Examples
I.1 Spider Template
Overall Description Template:
{db id} contains tables such as {table1 name}, {
table2 name}
Primary Key Template:
{primary key} is the primary key.
Table Description Template:
Table {table name} has column such as {column 1
name}, {column 2 name}, ...
Foreign Keys Description Template:
The {column1 name} of {table 1} is the foreign
key of {column2 name} of {table 2}
I.2 TabFact Template
Template Examples:
Table 1-24143253-5:
{name} lost his spouse {deceased spouse} to {
cause of death} on {date of spouses death} after
{length of marriage} of marriage; they had {
children together} together; he is currently {
current marital status}Table 2-14978398-2:
The {version} of song Comme j /quotesingle.Varai mal has a
length of {length} in album {album} remixed by {
remixed by} in year {year}
Table 1-15187735-12:
On {date} in 1936 VFL Season, the home team {
home team} and away team {away team} had a game
at venue {venue} with a crowd of {crowd}; the
home team score is {home team score} and the
away team score is {away team score}
I.3 WikiSQL Template
Template Example:
Table 1-14240688-1:
in {year} were in division {division}, {league}
ranked {regular season}, made it to {playoffs}
of the playoffs, made it to <{open cup}> in the
open cup, and kept an average attendance of {avg
attendance}
Table 2-12997882-1:
On {date} in 2008 European Figure Skating, the
home team {home team} and away team {away team}
had a game at venue {venue} with a crowd of {
crowd}; the home team score is {home team score}
and the away team score is {away team score}
Table 1-13740746-1:
Episode number {ep no} of gerry anderson /quotesingle.Vars new
captain scarlet with a title of {title} is
directed by {director} and written by {written
by}; its original air date is {original air date
}; the production number is {production no}
INFO:root:--------------------
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:The abstract of the paper titled "UNIFIED SKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models" is as follows:

Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UNIFIED SKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UNIFIED SKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UNIFIED SKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UNIFIED SKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UNIFIED SKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.
INFO:root:Done
INFO:root:[green]User: 
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:### Abstract
Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UNIFIED SKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UNIFIED SKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UNIFIED SKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UNIFIED SKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UNIFIED SKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.

### Summary of the Paper
The paper introduces the UNIFIED SKG framework, which aims to unify various structured knowledge grounding (SKG) tasks into a coherent text-to-text format. The authors identify that SKG tasks have been studied in isolation by different research communities, leading to a lack of systematic research and compatibility across tasks. By proposing a unified framework, the authors aim to promote comprehensive research in SKG.

Key contributions of the paper include:
1. **Unification of 21 SKG Tasks**: The authors categorize and standardize 21 different SKG tasks into a single framework, allowing for easier benchmarking and comparison.
2. **Benchmarking with T5**: The paper demonstrates that the T5 model, when fine-tuned on these tasks, achieves state-of-the-art performance across nearly all tasks. The authors also explore the impact of model size on performance.
3. **Multi-task Learning**: The authors show that multi-task prefix-tuning significantly enhances performance across most tasks, indicating that knowledge sharing among tasks can lead to better outcomes.
4. **Zero-shot and Few-shot Learning**: The paper investigates the performance of various models (T0, GPT-3, Codex) in zero-shot and few-shot learning scenarios, revealing that these models struggle with SKG tasks.
5. **Controlled Experiments**: The authors conduct controlled experiments to analyze the effects of different structured knowledge encoding methods on SKG tasks.

The paper concludes by emphasizing the potential of the UNIFIED SKG framework to facilitate future research in structured knowledge grounding, enabling the design of new models and learning algorithms that can generalize across diverse SKG tasks. The framework is open-sourced to encourage community engagement and further development in this area.
INFO:root:Done
INFO:root:[green]User: 
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:{"name":"google_search", "arguments":{"query":"latest news in USA"}}

INFO:root:Calling function: google_search ...
INFO:root:Done
INFO:root:--------------------
INFO:root:SearchResult(url=https://www.cnn.com/, title=CNN: Breaking News, Latest News and Videos, description=View the latest news and breaking news today for U.S., world, weather, entertainment, politics and health at CNN.com.)

SearchResult(url=https://www.nbcnews.com/, title=NBC News - Breaking News & Top Stories - Latest World, US ..., description=Inflation cools to lowest level since February 2021 · Judge delays trial for Idaho college murders suspect to August 2025 · Ethel Kennedy, matriarch of the famous ...)

SearchResult(url=https://www.usatoday.com/, title=USA TODAY - Breaking News and Latest News Today, description=12 rescued from former Colorado gold mine after fatality during tour · Fourth person infected in California as bird flu outbreak spreads · Police find fentanyl ...)

SearchResult(url=https://abcnews.go.com/, title=ABC News - Breaking News, Latest News and Videos, description=Obama starts campaign blitz for Harris in critical battleground Pennsylvania · Republicans in storm states face 'balancing act' on response so close to election ...)

SearchResult(url=https://www.cbsnews.com/, title=CBS News - Breaking news, 24/7 live streaming news & top ..., description=MoneyWatch ; TD Bank to pay $3 billion after breaking U.S. money laundering rules · 8H ago ; U.S. inflation continued to cool in September, latest CPI data shows.)

SearchResult(url=https://www.reuters.com/world/us/, title=Latest U.S. News | Top headlines from the USA, description=Reuters.com is your online source for the latest US news stories and current events, ensuring our readers up to date with any breaking news developments.)

SearchResult(url=https://www.foxnews.com/, title=Fox News - Breaking News Updates | Latest News Headlines ..., description=Breaking News, Latest News and Current News from FOXNews.com. Breaking news ... Hurricane Helene triggers growing crisis at hospitals across the United States.)

SearchResult(url=https://news.google.com/, title=Google News, description=CNN offers to host live town halls with Trump and Harris · Trump rejects Fox News invite to debate Harris in late October · Deadline for second Trump-Harris ...)

SearchResult(url=https://www.msnbc.com/, title=MSNBC News - Breaking News and News Today | Latest News, description=MSNBC breaking news and the latest news for today. Get daily news from local news reporters and world news updates with live audio & video from our team.)

SearchResult(url=https://www.washingtonpost.com/, title=The Washington Post - Breaking news and latest headlines ..., description=Subscribe for the latest on U.S. and international news, politics, business, technology, climate change, health and wellness, sports, science, weather,)
INFO:root:--------------------
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:Here are some sources for the latest news in the USA:

1. [CNN](https://www.cnn.com/) - Breaking news and latest updates on various topics including U.S. news, world news, weather, entertainment, politics, and health.

2. [NBC News](https://www.nbcnews.com/) - Features breaking news stories, including updates on inflation, legal cases, and other significant events.

3. [USA Today](https://www.usatoday.com/) - Offers news on various topics including recent rescues, health outbreaks, and police investigations.

4. [ABC News](https://abcnews.go.com/) - Covers political campaigns, election news, and responses to natural disasters.

5. [CBS News](https://www.cbsnews.com/) - Provides updates on financial news, inflation data, and other current events.

6. [Reuters](https://www.reuters.com/world/us/) - An online source for the latest U.S. news stories and current events.

7. [Fox News](https://www.foxnews.com/) - Offers breaking news updates and current headlines.

8. [Google News](https://news.google.com/) - Aggregates news from various sources, including political updates and significant events.

9. [MSNBC](https://www.msnbc.com/) - Provides breaking news and updates on various topics.

10. [The Washington Post](https://www.washingtonpost.com/) - Covers U.S. and international news, politics, business, and more.

You can visit these links for the most current news updates.
INFO:root:Done
INFO:root:[green]User: 
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:{"name":"google_search", "arguments":{"query":"latest headlines site:washingtonpost.com"}}

INFO:root:Calling function: google_search ...
INFO:root:Done
INFO:root:--------------------
INFO:root:SearchResult(url=https://www.washingtonpost.com/, title=The Washington Post - Breaking news and latest headlines ..., description=Breaking news, live coverage, investigations, analysis, video, photos and opinions from The Washington Post. Subscribe for the latest on U.S. and ...)

SearchResult(url=https://www.washingtonpost.com/latest-headlines/, title=Latest From The Post, description=Breaking news, live coverage, the latest headlines from The Washington Post. Subscribe for the latest on U.S. and international news, politics, business, ...)

SearchResult(url=https://www.washingtonpost.com/newsletters/todays-headlines/, title=Today's Headlines - Newsletters & Email Alerts, description=The most important news stories of the day, curated by Post editors, delivered every morning.)

SearchResult(url=https://www.washingtonpost.com/?de7e1ac7_page=7&ed856a68_page=4, title=Breaking news and latest headlines, U.S. news, world ..., description=Sep 16, 2024 — Breaking news, live coverage, investigations, analysis, video, photos and opinions from The Washington Post. Subscribe for the latest on ...)

SearchResult(url=http://www.washingtonpost.com/?utm_medium=email&utm_source=newsletter&wpisrc=nl_weather&utm_campaign=wp_cwg_forecast_weekdays&carta-url=https%3A%2F%2Fs2.washingtonpost.com%2Fcar-ln-tr%2F1d12695%2F665edead25eb8f379a8d064e%2F%2F19%2F26%2F, title=Breaking news and latest headlines, U.S. ..., description=By Drew Harwell. WAR IN UKRAINE. Employees of the Russia Today television network wait for the arrival of Vladimir Putin in 2013. (AFP/Getty Images) ...)

SearchResult(url=http://www.washingtonpost.com/?utm_campaign=wp_politics_am&utm_medium=email&utm_source=newsletter&wpisrc=nl_politics&carta-url=https%3A%2F%2Fs2.washingtonpost.com%2Fcar-ln-tr%2F1d21f0e%2F66f14013ea7afa4c7194373a%2F6286344a9110416a15c2ee90%2F3%2F49%2F66f14013ea7afa4c7194373a, title=Breaking news and latest headlines, U.S. ..., description=Top Stories. Latest. For You. LIVE. Election 2024 · Middle East conflict. TRENDING. Trump's rallies · Mike Lawler blackface · Haiti gang attack · Garth Brooks.)

SearchResult(url=http://washingtonpost.com/?itid=lk_inline_manual_213, title=The Washington Post - Breaking news and latest headlines, U.S. ..., description=Breaking news, live coverage, investigations, analysis, video, photos and opinions from The Washington Post. Subscribe for the latest on U.S. and ...)

SearchResult(url=https://www.washingtonpost.com/national-security/2024/10/10/trump-jan6-jack-smith-appendix-public-release/, title=Judge approves releasing source documents with more ..., description=12 minutes ago — Judge Tanya Chutkan gave Trump seven days to weigh litigation to block the release of the special counsel's source documents in his election ...)

SearchResult(url=https://www.washingtonpost.com/coronavirus/, title=Coronavirus, description=News and updates about the coronavirus pandemic: Cases in the US, death toll, what you need to know about the virus, how to prepare, how to get tested.)

SearchResult(url=https://www.washingtonpost.com/elections/2024/10/10/trump-harris-cbs-license-60-minutes/, title=Trump calls for CBS to lose license over Harris '60 Minutes ..., description=4 hours ago — After Trump's criticism of the editing of a “60 Minutes” interview, the FCC chairwoman said the former president's “threats against free ...)

SearchResult(url=https://www.washingtonpost.com/politics/2024/10/10/hurricane-helene-republicans-trump/, title=How hurricane falsehoods are dividing the Republican Party, description=6 hours ago — As the country digs out, Republicans in storm-battered states appear torn between the need to curb dangerous misinformation and fear of ...)

SearchResult(url=https://www.washingtonpost.com/politics/2024/10/09/fox-news-quietly-reports-fact-sheet-correcting-fox-news-misinformation/, title=Fox News quietly reports on a fact sheet correcting ..., description=1 day ago — In the channel's eternal struggle between helping Donald Trump and informing its audience, Trump once again prevailed in the aftermath of ...)
INFO:root:--------------------
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:Here are some of the latest headlines from The Washington Post:

1. **Judge approves releasing source documents with more details on Trump’s January 6 case** - A judge has given Trump seven days to weigh litigation to block the release of the special counsel's source documents in his election-related case.

2. **Trump calls for CBS to lose license over Harris '60 Minutes' interview** - After Trump's criticism of the editing of a “60 Minutes” interview, the FCC chairwoman responded to his threats against free speech.

3. **How hurricane falsehoods are dividing the Republican Party** - As the country recovers from recent storms, Republicans in affected states are caught between addressing misinformation and political fears.

4. **Fox News quietly reports on a fact sheet correcting misinformation** - The network struggles to balance support for Trump while providing accurate information to its audience.

For more details, you can visit [The Washington Post](https://www.washingtonpost.com/).
INFO:root:Done
INFO:root:[green]User: 
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:{"name":"google_search", "arguments":{"query":"Tesla We Robot event date"}}

INFO:root:Calling function: google_search ...
INFO:root:Done
INFO:root:--------------------
INFO:root:SearchResult(url=https://techcrunch.com/storyline/tesla-robotaxi-event-follow-elon-musks-big-reveals-here/, title=Tesla We, Robot event: Everything Elon Musk just showed, ..., description=2 hours ago — Elon Musk unveils the Robovan: the biggest surprise from Tesla's We, Robot event · Tesla reveals 20 Cybercabs at We, Robot event, says you'll be ...)

SearchResult(url=https://www.theverge.com/2024/10/10/24265122/tesla-robotaxi-event-how-to-watch-what-to-expect-optimus, title=Tesla robotaxi event: how to watch and what to expect, description=19 hours ago — Tesla's “We, Robot” robotaxi presentation will take place on Thursday, October 10th, at 10PM ET / 9PM CT / 7PM PT. How to watch Tesla's Robotaxi ...)

SearchResult(url=https://www.reddit.com/r/TeslaLounge/comments/1g086ox/teslas_we_robot_robotaxi_event_megathread/, title=Tesla's "We, Robot" Robotaxi Event Megathread, description=Tesla is hosting their Robotaxi "We, Robot" event live on X on October 10th at 7pm PT, 8pm MT, 9pm CT, 10pm ET, 2am UTC next day early morning.)

SearchResult(url=https://www.shacknews.com/article/141739/tesla-we-robot-october-2024-livestream, title=Watch the Tesla We, Robot October 10, 2024 livestream here, description=8 hours ago — Presumed to be the Robotaxi event, today's Tesla We, Robot livestream is scheduled to begin at 7:30 p.m. PT / 10:30 p.m. ET on October 10, 2024.)

SearchResult(url=https://www.notateslaapp.com/news/2311/tesla-we-robot-robotaxi-event-how-and-where-to-watch, title=Tesla 'We, Robot' Robotaxi Event: How and Where to ..., description=14 hours ago — Tesla's Robotaxi event—We, Robot—is right around the corner. It takes place this Thursday, October 10th. Invites …)

SearchResult(url=https://techcrunch.com/2024/10/09/tesla-robotaxi-event-how-to-watch-the-reveal-thursday/, title=Tesla Robotaxi event: How to watch the reveal Thursday, description=2 days ago — Musk first teased the Robotaxi event in April, setting a date of August 8 for a reveal — a date that Tesla had to push back due to “an important ...)

SearchResult(url=https://electriccarsreport.com/2024/10/tesla-unveils-robotaxi-and-robovan-at-we-robot-event/, title=Tesla Unveils Robotaxi and Robovan at "We, Robot" Event, description=6 minutes ago — Elon Musk showcased Tesla's highly anticipated driverless taxi, the Robotaxi, at the company's “We, Robot” event on Thursday evening.)

SearchResult(url=https://www.theverge.com/2024/10/10/24265530/tesla-robotaxi-elon-musk-features-range-price-release-date, title=Tesla Cybercab announced: Elon Musk's robotaxi is finally ..., description=3 hours ago — Additionally, Tesla is developing the Optimus robot, which could be available for $20,000-$30,000, and is capable of performing various tasks. “ ...)

SearchResult(url=https://www.usatoday.com/story/money/cars/2024/10/10/tesla-robotaxi-event-debut/75614437007/, title=Tesla set to unveil robotaxi in 'We, Robot' event, description=8 hours ago — Tesla appears set to unveil its self-driving taxi during the electric car giant's "We, Robot" event Thursday night.)

SearchResult(url=https://www.npr.org/2024/10/10/nx-s1-5144675/tesla-robotaxi-cybercab-reveal-thursday, title=Tesla is expected to unveil a robotaxi tonight, description=21 hours ago — Tesla is calling the event “We, Robot.” At 7 p.m. Pacific time, Tesla CEO Elon Musk is expected to unveil the company's design for a ...)

SearchResult(url=https://techcrunch.com/2024/10/09/tesla-robotaxi-event-how-to-watch-the-reveal-thursday/, title=Tesla Robotaxi event: How to watch the reveal Thursday, description=2 days ago — Musk first teased the Robotaxi event in April, setting a date of August 8 for a reveal — a date that Tesla had to push back due to “an important ...)

SearchResult(url=https://www.businessinsider.com/tesla-robotaxi-event-cybercab-elon-musk-2024-10, title=Elon Musk reveals Tesla's Robotaxi and Robovan, description=2 hours ago — Elon Musk just unveiled Tesla's Robotaxi at the company's "We, Robot" event in Los Angeles. ... date due to Musk requesting an "important ...)
INFO:root:--------------------
INFO:root:assign_2_scholar is thinking...
INFO:root:Done
INFO:root:[blue]assign_2_scholar: 
INFO:root:Tesla's "We, Robot" event took place on **October 10, 2024**. During this event, Elon Musk unveiled the highly anticipated Robotaxi and Robovan.
INFO:root:Done
INFO:root:[green]User: 
INFO:root:
[red]Bye!
